{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM/3XkwytR43JypvsdiFaBc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sku1978/sk-share-repo/blob/main/Spark/SparkDataFrame/SparkDataFrameNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhiBxUATKbVB"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq  > /dev/null \n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz > /dev/null \n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5veJKNsaD8gv"
      },
      "source": [
        "!mkdir /content/conf /content/lib\n",
        "!wget -O /content/conf/log4j.properties https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/conf/log4j.properties > /dev/null 2>&1\n",
        "!mv /content/spark-3.1.1-bin-hadoop3.2/conf/spark-defaults.conf /content/spark-3.1.1-bin-hadoop3.2/conf/spark-defaults.conf.bk  > /dev/null 2>&1\n",
        "!wget -O /content/spark-3.1.1-bin-hadoop3.2/conf/spark-defaults.conf https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/conf/spark-defaults.conf  > /dev/null 2>&1\n",
        "!wget -O /content/conf/spark.conf https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/conf/spark.conf > /dev/null 2>&1\n",
        "\n",
        "!wget -O /content/lib/logger.py https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/lib/logger.py  > /dev/null 2>&1\n",
        "!wget -O /content/lib/utils.py https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/lib/utils.py  > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVA-bBFsCPyz"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe_e2IaqXMTC"
      },
      "source": [
        "**Spark UI section**\n",
        "<br>To use Spark UI, uncomment below sections and use the public link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhLCqy4a6ihE"
      },
      "source": [
        "#!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "#!unzip ngrok-stable-linux-amd64.zip\n",
        "#get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSxhuV046is1"
      },
      "source": [
        "#!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PybcmfVuWXgc"
      },
      "source": [
        "**Main Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jim-sE5KqDe"
      },
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark import SparkConf, SparkFiles\n",
        "from lib.logger import Log4J\n",
        "from lib.utils import get_spark_app_config\n",
        "\n",
        "conf=get_spark_app_config()\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .config(conf=conf)\\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()\n",
        "\n",
        "logger = Log4J(spark)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNbHk56QWhH0"
      },
      "source": [
        "**Basic CSV Read**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPD-t1kD5bEI"
      },
      "source": [
        "def load_csv_file(spark, url):\n",
        "   spark.sparkContext.addFile(url)\n",
        "\n",
        "   survey_df=spark.read \\\n",
        "   .format(\"csv\") \\\n",
        "   .option(\"header\", \"true\") \\\n",
        "   .option(\"inferSchema\", \"true\") \\\n",
        "   .option(\"mode\", \"FAILFAST\") \\\n",
        "   .load('file://'+SparkFiles.get(\"sample.csv\"))\n",
        "\n",
        "   return survey_df\n",
        "\n",
        "def count_by_country(survey_df):\n",
        "  count_df= survey_df.select(\"Age\", \"Gender\", \"Country\", \"State\") \\\n",
        "                     .where(\"Age <= 40\") \\\n",
        "                     .groupBy(\"Country\") \\\n",
        "                     .count()\n",
        "  return count_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSfkRz_HWQL-"
      },
      "source": [
        "logger.info(\"Start CSV Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/sample.csv'\n",
        "\n",
        "survey_df=load_csv_file(spark, url)\n",
        "partitioned_survey_df=survey_df.repartition(2)\n",
        "\n",
        "count_df=count_by_country(partitioned_survey_df)\n",
        "\n",
        "logger.info(count_df.collect())\n",
        "\n",
        "logger.info(\"End CSV Load\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgrObFdrXOso"
      },
      "source": [
        "**Data Types**<br>\n",
        "(https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fy1u9idX4yC"
      },
      "source": [
        "**CSV Read (CSV)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhAtshrmYDDh"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, StringType\n",
        "\n",
        "logger.info(\"Start Flight Time CSV Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flightSchemaStruct = StructType([StructField(\"FL_DATE\", DateType(), True),\n",
        "                                 StructField(\"OP_CARRIER\", StringType(), True),\n",
        "                                 StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), True),\n",
        "                                 StructField(\"ORIGIN\", StringType(), True),\n",
        "                                 StructField(\"ORIGIN_CITY_NAME\", StringType(), True),\n",
        "                                 StructField(\"DEST\", StringType(), True),\n",
        "                                 StructField(\"DEST_CITY_NAME\", StringType(), True),\n",
        "                                 StructField(\"CRS_DEP_TIME\", IntegerType(), True),\n",
        "                                 StructField(\"DEP_TIME\", IntegerType(), True),\n",
        "                                 StructField(\"WHEELS_ON\", IntegerType(), True),\n",
        "                                 StructField(\"TAXI_IN\", IntegerType(), True),\n",
        "                                 StructField(\"CRS_ARR_TIME\", IntegerType(), True),\n",
        "                                 StructField(\"ARR_TIME\", IntegerType(), True),\n",
        "                                 StructField(\"CANCELLED\", IntegerType(), True),\n",
        "                                 StructField(\"DISTANCE\", StringType(), True),\n",
        "                                ])\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"dateFormat\", \"M/d/y\") \\\n",
        "                    .schema(flightSchemaStruct) \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.csv\"))\n",
        "\n",
        "flight_time_df.show()\n",
        "\n",
        "logger.info(\"End CSV Load\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2uWblhdVR6Q"
      },
      "source": [
        "**JSON Read**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZYy8bFcVUYP"
      },
      "source": [
        "logger.info(\"Start Flight Time JSON Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.json'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flightSchemaDDL = \"\"\"FL_DATE DATE, \n",
        "                     OP_CARRIER STRING, \n",
        "                     OP_CARRIER_FL_NUM INT,\n",
        "                     ORIGIN STRING,\n",
        "                     ORIGIN_CITY_NAME STRING,\n",
        "                     DEST STRING,\n",
        "                     DEST_CITY_NAME STRING,\n",
        "                     CRS_DEP_TIME INT,\n",
        "                     DEP_TIME INT,\n",
        "                     WHEELS_ON INT,\n",
        "                     TAXI_IN INT,\n",
        "                     CRS_ARR_TIME INT,\n",
        "                     ARR_TIME INT,\n",
        "                     CANCELLED INT,\n",
        "                     DISTANCE STRING\"\"\"\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"json\") \\\n",
        "                    .option(\"dateFormat\", \"M/d/y\") \\\n",
        "                    .schema(flightSchemaDDL) \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.json\"))\n",
        "\n",
        "flight_time_df.show()\n",
        "\n",
        "logger.info(\"End JSON Load\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQiEpwe1V2ii"
      },
      "source": [
        "**Parquet Read**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zW8xC5BV48M"
      },
      "source": [
        "logger.info(\"Start Flight Time Parquet Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.parquet\"))\n",
        "\n",
        "flight_time_df.show()\n",
        "\n",
        "logger.info(\"End Parquet Load\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPHyggQ0l_fL"
      },
      "source": [
        "**Write Avro file**<br>\n",
        "Added <br>\n",
        "spark.jars.packages                org.apache.spark:spark-avro_2.12:3.0.0\n",
        "<br>to<br>\n",
        "conf/spark-defaults.conf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwGZu8xamR5o"
      },
      "source": [
        "from pyspark.sql.functions  import spark_partition_id\n",
        "\n",
        "logger.info(\"Start Flight Time Parquet Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.parquet\"))\n",
        "\n",
        "logger.info(\"End Parquet Load\")\n",
        "logger.info(\"Number of partitions: \" + str(flight_time_df.rdd.getNumPartitions()))\n",
        "#Even though there are two partitions, since the records are less, only one partition is used. Can repartition to get more partitions in o/p\n",
        "logger.info(\"Records per partition: \" + str(flight_time_df.groupBy(spark_partition_id()).count().collect()))\n",
        "\n",
        "logger.info(\"Start Avro write\")\n",
        "\n",
        "flight_time_df.write \\\n",
        "              .format(\"avro\") \\\n",
        "              .mode(\"overwrite\") \\\n",
        "              .option(\"path\", \"dataSink/avro/\") \\\n",
        "              .save()\n",
        "\n",
        "logger.info(\"End Avro write\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pkC1hvOncFm"
      },
      "source": [
        "!ls -l dataSink/avro/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G26doANBtA8V"
      },
      "source": [
        "**Partioned Write**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3QO6abKtFBO"
      },
      "source": [
        "logger.info(\"Start Flight Time Parquet Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.parquet\"))\n",
        "\n",
        "logger.info(\"Start Partitioned JSON write\")\n",
        "\n",
        "#To check file split at 10K records, look for this example\n",
        "#!wc -l dataSink/json/OP_CARRIER\\=DL/ORIGIN\\=ATL/*\n",
        "flight_time_df.write \\\n",
        "              .format(\"json\") \\\n",
        "              .mode(\"overwrite\") \\\n",
        "              .option(\"path\", \"dataSink/json/\") \\\n",
        "              .partitionBy(\"OP_CARRIER\", \"ORIGIN\") \\\n",
        "              .option(\"maxRecordsPerFile\", 10000) \\\n",
        "              .save()\n",
        "\n",
        "logger.info(\"End Partitioned JSON write\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCk0ylYEta56"
      },
      "source": [
        "!ls -l dataSink/json/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNZG1LMBmNpM"
      },
      "source": [
        "**Managed tables**<br>\n",
        "Config has been setup to write tables into *warehouse_location* folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vstXvCmGmQf_"
      },
      "source": [
        "logger.info(\"Start Flight Time Parquet Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.parquet\"))\n",
        "\n",
        "logger.info(\"Create database\")\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS AIRLINES_DB\")\n",
        "spark.catalog.setCurrentDatabase(\"AIRLINES_DB\")\n",
        "\n",
        "logger.info(\"Write table\")\n",
        "flight_time_df.write \\\n",
        "              .mode(\"overwrite\") \\\n",
        "              .bucketBy(5,\"OP_CARRIER\", \"ORIGIN\") \\\n",
        "              .sortBy(\"OP_CARRIER\", \"ORIGIN\") \\\n",
        "              .saveAsTable(\"flight_data_tbl\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM AIRLINES_DB.flight_data_tbl\").show(10)\n",
        "\n",
        "logger.info(spark.catalog.listTables(\"AIRLINES_DB\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXZDz6YQokWj"
      },
      "source": [
        "!ls -lR spark-warehouse/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpAfKwCkM8wf"
      },
      "source": [
        "**Log File/Raw Data/Unstructured Data handling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI-GqcvjM79E"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "logger.info(\"Start Log file load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/apache_logs.txt'\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "raw_df=spark.read \\\n",
        "                    .format(\"text\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"apache_logs.txt\"))\n",
        "\n",
        "raw_df.printSchema()\n",
        "\n",
        "log_reg = r'^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\S+) \"(\\S+)\" \"([^\"]*)'\n",
        "\n",
        "logs_df = raw_df.select(regexp_extract('value', log_reg, 1).alias('ip'),\n",
        "                        regexp_extract('value', log_reg, 4).alias('date'),\n",
        "                        regexp_extract('value', log_reg, 6).alias('request'),\n",
        "                        regexp_extract('value', log_reg, 10).alias('referrer'))\n",
        "logs_df.printSchema()\n",
        "\n",
        "logs_df.withColumn('referrer', substring_index('referrer', '/',3)) \\\n",
        "       .where(\"trim(referrer) != '-' \") \\\n",
        "       .groupBy('referrer') \\\n",
        "       .count() \\\n",
        "       .show(100, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7d-BQDWVMZ7"
      },
      "source": [
        "**Handcrafted DataFrame creating Rows**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIVxtSHGVV1e"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "my_schema=StructType([StructField(\"ID\", StringType(), True),\n",
        "                      StructField(\"EventDate\", StringType(), True)])\n",
        "\n",
        "my_rows=[Row(\"123\", \"21/03/2019\"), \n",
        "         Row(\"234\", \"21/05/2029\"), \n",
        "         Row(\"345\", \"01/02/2020\"),\n",
        "         Row(\"456\", \"09/12/2018\"),\n",
        "         Row(\"567\", \"03/06/2019\"),]\n",
        "\n",
        "my_rdd=spark.sparkContext.parallelize(my_rows, 2)\n",
        "\n",
        "my_df=spark.createDataFrame(my_rdd, my_schema)\n",
        "\n",
        "my_df.printSchema()\n",
        "my_df.show()\n",
        "\n",
        "new_df=my_df.withColumn('EventDate', to_date('EventDate', 'd/M/y'))\n",
        "\n",
        "new_df.printSchema()\n",
        "new_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELqg2_QzKjYP"
      },
      "source": [
        "**Column Expressions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81r9iL0BKlEw"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, StringType\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "logger.info(\"Start CSV Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/databricks-airlines.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "airlines_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .option(\"samplingRatio\", \"0.5\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"databricks-airlines.csv\"))\n",
        "\n",
        "#shows three ways to refer to columns\n",
        "airlines_df.select(\"Origin\", col(\"Dest\"), airlines_df.IsArrDelayed).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71qFJoQ1OBl9"
      },
      "source": [
        "airlines_df.select(\"Origin\", \"Dest\", \"Distance\", \"Year\", \"Month\", \"DayOfMonth\").show(10)\n",
        "\n",
        "airlines_df.select(\"Origin\", \"Dest\", \"Distance\", expr(\"to_date(concat(Year, Month, DayOfMonth), 'yyyyMMdd') as FlightDate\")).show(10)\n",
        "\n",
        "airlines_df.select(\"Origin\", \"Dest\", \"Distance\", to_date(concat(\"Year\", \"Month\", \"DayOfMonth\"), 'yyyyMMdd').alias(\"FlightDate\")).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4hzYCkkWVVX"
      },
      "source": [
        "**User Defined Function (UDF)**<br>\n",
        "to be used as column expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRisIJlAXfNL"
      },
      "source": [
        "import re\n",
        "def parse_gender(gender):\n",
        "  female_pattern = r'^f$|f.m|w.m'\n",
        "  male_pattern = r'^m$|ma|m.l'\n",
        "\n",
        "  if re.search(female_pattern, gender.lower()):\n",
        "    return \"Female\"\n",
        "  elif re.search(male_pattern, gender.lower()):\n",
        "    return \"Male\"\n",
        "  else:\n",
        "    return \"Unknown\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLhUj9sHWY6O"
      },
      "source": [
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/survey.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "survey_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"survey.csv\"))\n",
        "\n",
        "survey_df.show(10)\n",
        "\n",
        "#Column object UDF (no entry in catalog)\n",
        "parse_gender_udf=udf(parse_gender, StringType())\n",
        "survey_df2=survey_df.withColumn(\"Gender\", parse_gender_udf(\"Gender\"))\n",
        "survey_df2.show(10)\n",
        "\n",
        "#SQL UDF (entry goes into catalog)\n",
        "spark.udf.register(\"parse_gender_udf\", parse_gender, StringType())\n",
        "survey_df3=survey_df.withColumn(\"Gender\", expr(\"parse_gender_udf(Gender)\"))\n",
        "survey_df3.show(10)\n",
        "\n",
        "spark.catalog.listFunctions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piuP7dIZXBM5"
      },
      "source": [
        "**View Log**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRtr-fRE5_3w"
      },
      "source": [
        "!cat app-logs/sparklog.log"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}