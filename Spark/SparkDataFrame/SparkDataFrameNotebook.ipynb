{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPhGY3i7MpMbsDd152dqa4c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sku1978/sk-share-repo/blob/main/Spark/SparkDataFrame/SparkDataFrameNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhiBxUATKbVB"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq  > /dev/null \n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz > /dev/null \n",
        "!pip install -q findspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5veJKNsaD8gv"
      },
      "source": [
        "!mkdir /content/conf /content/lib\n",
        "!wget -O /content/conf/log4j.properties https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/conf/log4j.properties > /dev/null 2>&1\n",
        "!mv /content/spark-3.1.1-bin-hadoop3.2/conf/spark-defaults.conf /content/spark-3.1.1-bin-hadoop3.2/conf/spark-defaults.conf.bk  > /dev/null 2>&1\n",
        "!wget -O /content/spark-3.1.1-bin-hadoop3.2/conf/spark-defaults.conf https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/conf/spark-defaults.conf  > /dev/null 2>&1\n",
        "!wget -O /content/conf/spark.conf https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/conf/spark.conf > /dev/null 2>&1\n",
        "\n",
        "!wget -O /content/lib/logger.py https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/lib/logger.py  > /dev/null 2>&1\n",
        "!wget -O /content/lib/utils.py https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/lib/utils.py  > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVA-bBFsCPyz"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe_e2IaqXMTC"
      },
      "source": [
        "**Spark UI section**\n",
        "<br>To use Spark UI, uncomment below sections and use the public link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhLCqy4a6ihE"
      },
      "source": [
        "#!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "#!unzip ngrok-stable-linux-amd64.zip\n",
        "#get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSxhuV046is1"
      },
      "source": [
        "#!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PybcmfVuWXgc"
      },
      "source": [
        "**Main Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jim-sE5KqDe"
      },
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark import SparkConf, SparkFiles\n",
        "from lib.logger import Log4J\n",
        "from lib.utils import get_spark_app_config\n",
        "\n",
        "conf=get_spark_app_config()\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .config(conf=conf)\\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()\n",
        "\n",
        "logger = Log4J(spark)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNbHk56QWhH0"
      },
      "source": [
        "**Basic CSV Read**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPD-t1kD5bEI"
      },
      "source": [
        "def load_csv_file(spark, url):\n",
        "   spark.sparkContext.addFile(url)\n",
        "\n",
        "   survey_df=spark.read \\\n",
        "   .format(\"csv\") \\\n",
        "   .option(\"header\", \"true\") \\\n",
        "   .option(\"inferSchema\", \"true\") \\\n",
        "   .option(\"mode\", \"FAILFAST\") \\\n",
        "   .load('file://'+SparkFiles.get(\"sample.csv\"))\n",
        "\n",
        "   return survey_df\n",
        "\n",
        "def count_by_country(survey_df):\n",
        "  count_df= survey_df.select(\"Age\", \"Gender\", \"Country\", \"State\") \\\n",
        "                     .where(\"Age <= 40\") \\\n",
        "                     .groupBy(\"Country\") \\\n",
        "                     .count()\n",
        "  return count_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSfkRz_HWQL-"
      },
      "source": [
        "logger.info(\"Start CSV Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/sample.csv'\n",
        "\n",
        "survey_df=load_csv_file(spark, url)\n",
        "partitioned_survey_df=survey_df.repartition(2)\n",
        "\n",
        "count_df=count_by_country(partitioned_survey_df)\n",
        "\n",
        "logger.info(count_df.collect())\n",
        "\n",
        "logger.info(\"End CSV Load\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgrObFdrXOso"
      },
      "source": [
        "**Data Types**<br>\n",
        "(https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fy1u9idX4yC"
      },
      "source": [
        "**CSV Read (CSV)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhAtshrmYDDh"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, StringType\n",
        "\n",
        "logger.info(\"Start Flight Time CSV Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flightSchemaStruct = StructType([StructField(\"FL_DATE\", DateType(), True),\n",
        "                                 StructField(\"OP_CARRIER\", StringType(), True),\n",
        "                                 StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), True),\n",
        "                                 StructField(\"ORIGIN\", StringType(), True),\n",
        "                                 StructField(\"ORIGIN_CITY_NAME\", StringType(), True),\n",
        "                                 StructField(\"DEST\", StringType(), True),\n",
        "                                 StructField(\"DEST_CITY_NAME\", StringType(), True),\n",
        "                                 StructField(\"CRS_DEP_TIME\", IntegerType(), True),\n",
        "                                 StructField(\"DEP_TIME\", IntegerType(), True),\n",
        "                                 StructField(\"WHEELS_ON\", IntegerType(), True),\n",
        "                                 StructField(\"TAXI_IN\", IntegerType(), True),\n",
        "                                 StructField(\"CRS_ARR_TIME\", IntegerType(), True),\n",
        "                                 StructField(\"ARR_TIME\", IntegerType(), True),\n",
        "                                 StructField(\"CANCELLED\", IntegerType(), True),\n",
        "                                 StructField(\"DISTANCE\", StringType(), True),\n",
        "                                ])\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"dateFormat\", \"M/d/y\") \\\n",
        "                    .schema(flightSchemaStruct) \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.csv\"))\n",
        "\n",
        "flight_time_df.show()\n",
        "\n",
        "logger.info(\"End CSV Load\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2uWblhdVR6Q"
      },
      "source": [
        "**JSON Read**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZYy8bFcVUYP"
      },
      "source": [
        "logger.info(\"Start Flight Time JSON Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.json'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flightSchemaDDL = \"\"\"FL_DATE DATE, \n",
        "                     OP_CARRIER STRING, \n",
        "                     OP_CARRIER_FL_NUM INT,\n",
        "                     ORIGIN STRING,\n",
        "                     ORIGIN_CITY_NAME STRING,\n",
        "                     DEST STRING,\n",
        "                     DEST_CITY_NAME STRING,\n",
        "                     CRS_DEP_TIME INT,\n",
        "                     DEP_TIME INT,\n",
        "                     WHEELS_ON INT,\n",
        "                     TAXI_IN INT,\n",
        "                     CRS_ARR_TIME INT,\n",
        "                     ARR_TIME INT,\n",
        "                     CANCELLED INT,\n",
        "                     DISTANCE STRING\"\"\"\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"json\") \\\n",
        "                    .option(\"dateFormat\", \"M/d/y\") \\\n",
        "                    .schema(flightSchemaDDL) \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.json\"))\n",
        "\n",
        "flight_time_df.show()\n",
        "\n",
        "logger.info(\"End JSON Load\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQiEpwe1V2ii"
      },
      "source": [
        "**Parquet Read**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zW8xC5BV48M"
      },
      "source": [
        "logger.info(\"Start Flight Time Parquet Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.parquet\"))\n",
        "\n",
        "flight_time_df.show()\n",
        "\n",
        "logger.info(\"End Parquet Load\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPHyggQ0l_fL"
      },
      "source": [
        "**Write Avro file**<br>\n",
        "Added <br>\n",
        "spark.jars.packages                org.apache.spark:spark-avro_2.12:3.0.0\n",
        "<br>to<br>\n",
        "conf/spark-defaults.conf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwGZu8xamR5o"
      },
      "source": [
        "from pyspark.sql.functions  import spark_partition_id\n",
        "\n",
        "logger.info(\"Start Flight Time Parquet Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.parquet\"))\n",
        "\n",
        "logger.info(\"End Parquet Load\")\n",
        "logger.info(\"Number of partitions: \" + str(flight_time_df.rdd.getNumPartitions()))\n",
        "#Even though there are two partitions, since the records are less, only one partition is used. Can repartition to get more partitions in o/p\n",
        "logger.info(\"Records per partition: \" + str(flight_time_df.groupBy(spark_partition_id()).count().collect()))\n",
        "\n",
        "logger.info(\"Start Avro write\")\n",
        "\n",
        "flight_time_df.write \\\n",
        "              .format(\"avro\") \\\n",
        "              .mode(\"overwrite\") \\\n",
        "              .option(\"path\", \"dataSink/avro/\") \\\n",
        "              .save()\n",
        "\n",
        "logger.info(\"End Avro write\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pkC1hvOncFm"
      },
      "source": [
        "!ls -l dataSink/avro/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G26doANBtA8V"
      },
      "source": [
        "**Partioned Write**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3QO6abKtFBO"
      },
      "source": [
        "logger.info(\"Start Flight Time Parquet Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.parquet\"))\n",
        "\n",
        "logger.info(\"Start Partitioned JSON write\")\n",
        "\n",
        "#To check file split at 10K records, look for this example\n",
        "#!wc -l dataSink/json/OP_CARRIER\\=DL/ORIGIN\\=ATL/*\n",
        "flight_time_df.write \\\n",
        "              .format(\"json\") \\\n",
        "              .mode(\"overwrite\") \\\n",
        "              .option(\"path\", \"dataSink/json/\") \\\n",
        "              .partitionBy(\"OP_CARRIER\", \"ORIGIN\") \\\n",
        "              .option(\"maxRecordsPerFile\", 10000) \\\n",
        "              .save()\n",
        "\n",
        "logger.info(\"End Partitioned JSON write\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCk0ylYEta56"
      },
      "source": [
        "!ls -l dataSink/json/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNZG1LMBmNpM"
      },
      "source": [
        "**Managed tables**<br>\n",
        "Config has been setup to write tables into *warehouse_location* folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vstXvCmGmQf_"
      },
      "source": [
        "logger.info(\"Start Flight Time Parquet Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.parquet\"))\n",
        "\n",
        "logger.info(\"Create database\")\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS AIRLINES_DB\")\n",
        "spark.catalog.setCurrentDatabase(\"AIRLINES_DB\")\n",
        "\n",
        "logger.info(\"Write table\")\n",
        "flight_time_df.write \\\n",
        "              .mode(\"overwrite\") \\\n",
        "              .bucketBy(5,\"OP_CARRIER\", \"ORIGIN\") \\\n",
        "              .sortBy(\"OP_CARRIER\", \"ORIGIN\") \\\n",
        "              .saveAsTable(\"flight_data_tbl\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM AIRLINES_DB.flight_data_tbl\").show(10)\n",
        "\n",
        "logger.info(spark.catalog.listTables(\"AIRLINES_DB\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXZDz6YQokWj"
      },
      "source": [
        "!ls -lR spark-warehouse/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpAfKwCkM8wf"
      },
      "source": [
        "**Log File/Raw Data/Unstructured Data handling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI-GqcvjM79E"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "logger.info(\"Start Log file load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/apache_logs.txt'\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "raw_df=spark.read \\\n",
        "                    .format(\"text\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"apache_logs.txt\"))\n",
        "\n",
        "raw_df.printSchema()\n",
        "\n",
        "log_reg = r'^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\S+) \"(\\S+)\" \"([^\"]*)'\n",
        "\n",
        "logs_df = raw_df.select(regexp_extract('value', log_reg, 1).alias('ip'),\n",
        "                        regexp_extract('value', log_reg, 4).alias('date'),\n",
        "                        regexp_extract('value', log_reg, 6).alias('request'),\n",
        "                        regexp_extract('value', log_reg, 10).alias('referrer'))\n",
        "logs_df.printSchema()\n",
        "\n",
        "logs_df.withColumn('referrer', substring_index('referrer', '/',3)) \\\n",
        "       .where(\"trim(referrer) != '-' \") \\\n",
        "       .groupBy('referrer') \\\n",
        "       .count() \\\n",
        "       .show(100, truncate=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7d-BQDWVMZ7"
      },
      "source": [
        "**Handcrafted DataFrame creating Rows**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIVxtSHGVV1e"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "my_schema=StructType([StructField(\"ID\", StringType(), True),\n",
        "                      StructField(\"EventDate\", StringType(), True)])\n",
        "\n",
        "my_rows=[Row(\"123\", \"21/03/2019\"), \n",
        "         Row(\"234\", \"21/05/2029\"), \n",
        "         Row(\"345\", \"01/02/2020\"),\n",
        "         Row(\"456\", \"09/12/2018\"),\n",
        "         Row(\"567\", \"03/06/2019\"),]\n",
        "\n",
        "my_rdd=spark.sparkContext.parallelize(my_rows, 2)\n",
        "\n",
        "my_df=spark.createDataFrame(my_rdd, my_schema)\n",
        "\n",
        "my_df.printSchema()\n",
        "my_df.show()\n",
        "\n",
        "new_df=my_df.withColumn('EventDate', to_date('EventDate', 'd/M/y'))\n",
        "\n",
        "new_df.printSchema()\n",
        "new_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELqg2_QzKjYP"
      },
      "source": [
        "**Column Expressions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81r9iL0BKlEw"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, StringType\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "logger.info(\"Start CSV Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/databricks-airlines.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "airlines_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .option(\"samplingRatio\", \"0.5\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"databricks-airlines.csv\"))\n",
        "\n",
        "#shows three ways to refer to columns\n",
        "airlines_df.select(\"Origin\", col(\"Dest\"), airlines_df.IsArrDelayed).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71qFJoQ1OBl9"
      },
      "source": [
        "airlines_df.select(\"Origin\", \"Dest\", \"Distance\", \"Year\", \"Month\", \"DayOfMonth\").show(10)\n",
        "\n",
        "airlines_df.select(\"Origin\", \"Dest\", \"Distance\", expr(\"to_date(concat(Year, Month, DayOfMonth), 'yyyyMMdd') as FlightDate\")).show(10)\n",
        "\n",
        "airlines_df.select(\"Origin\", \"Dest\", \"Distance\", to_date(concat(\"Year\", \"Month\", \"DayOfMonth\"), 'yyyyMMdd').alias(\"FlightDate\")).show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4hzYCkkWVVX"
      },
      "source": [
        "**User Defined Function (UDF)**<br>\n",
        "to be used as column expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRisIJlAXfNL"
      },
      "source": [
        "import re\n",
        "def parse_gender(gender):\n",
        "  female_pattern = r'^f$|f.m|w.m'\n",
        "  male_pattern = r'^m$|ma|m.l'\n",
        "\n",
        "  if re.search(female_pattern, gender.lower()):\n",
        "    return \"Female\"\n",
        "  elif re.search(male_pattern, gender.lower()):\n",
        "    return \"Male\"\n",
        "  else:\n",
        "    return \"Unknown\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLhUj9sHWY6O"
      },
      "source": [
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/survey.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "survey_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"survey.csv\"))\n",
        "\n",
        "survey_df.show(10)\n",
        "\n",
        "#Column object UDF (no entry in catalog)\n",
        "parse_gender_udf=udf(parse_gender, StringType())\n",
        "survey_df2=survey_df.withColumn(\"Gender\", parse_gender_udf(\"Gender\"))\n",
        "survey_df2.show(10)\n",
        "\n",
        "#SQL UDF (entry goes into catalog)\n",
        "spark.udf.register(\"parse_gender_udf\", parse_gender, StringType())\n",
        "survey_df3=survey_df.withColumn(\"Gender\", expr(\"parse_gender_udf(Gender)\"))\n",
        "survey_df3.show(10)\n",
        "\n",
        "spark.catalog.listFunctions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNT0rnKxeFt2"
      },
      "source": [
        "**Miscellaneous functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwRb8OEFeM-W"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, monotonically_increasing_id, when, expr\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "data_list = [(\"Ravi\", \"28\", \"1\", \"2002\"),\n",
        "             (\"Abdul\", \"23\", \"5\", \"81\"),  # 1981\n",
        "             (\"John\", \"12\", \"12\", \"6\"),  # 2006\n",
        "             (\"Rosy\", \"7\", \"8\", \"63\"),  # 1963\n",
        "             (\"Abdul\", \"23\", \"5\", \"81\")]  # 1981\n",
        "\n",
        "raw_df = spark.createDataFrame(data_list).toDF(\"name\", \"day\", \"month\", \"year\")\n",
        "raw_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0dOmyRMggFu"
      },
      "source": [
        "#Add a new column\n",
        "\n",
        "df1 = raw_df.withColumn(\"id\", monotonically_increasing_id())\n",
        "df1.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwuZUsnng87c"
      },
      "source": [
        "# CASE WHEN and CAST\n",
        "df2=df1.withColumn(\"year\", expr(\"\"\"\n",
        "                                CASE WHEN year < 21  THEN cast(year as int) + 2000\n",
        "                                     WHEN year < 100 THEN cast(year as int) + 1900\n",
        "                                     ELSE cast(year as int)\n",
        "                                END\n",
        "                                \"\"\"))\n",
        "df2.show()\n",
        "\n",
        "df2=df1.withColumn(\"year\", expr(\"\"\"\n",
        "                                CASE WHEN year < 21  THEN year + 2000\n",
        "                                     WHEN year < 100 THEN year + 1900\n",
        "                                     ELSE year\n",
        "                                END\n",
        "                                \"\"\").cast(IntegerType()))\n",
        "df2.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkcbiBpjOZL6"
      },
      "source": [
        "# Cast the fields and alternative WHEN\n",
        "df3=df1.withColumn(\"day\", col(\"day\").cast(IntegerType())) \\\n",
        "       .withColumn(\"month\", col(\"month\").cast(IntegerType())) \\\n",
        "       .withColumn(\"year\", col(\"year\").cast(IntegerType())) \\\n",
        "\n",
        "df3.printSchema()\n",
        "\n",
        "df4=df3.withColumn(\"year\", \\\n",
        "                          when(col(\"year\") < 21, col(\"year\") + 2000) \\\n",
        "                        .when(col(\"year\") < 100, col(\"year\") + 1900) \\\n",
        "                        .otherwise(col(\"year\")))\n",
        "df4.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRGb1DXCS7Z9"
      },
      "source": [
        "**Add/Remove columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SSgyxRnS-gd"
      },
      "source": [
        "df5=df2.withColumn(\"dob\", expr(\"to_date(concat(day, '/', month, '/', year), 'd/M/y')\")) \\\n",
        "       .drop(\"day\", \"month\", \"year\") \\\n",
        "       .dropDuplicates([\"name\", \"dob\"])\n",
        "\n",
        "df5.sort(df5.dob.desc()).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlVmPJ3YXEBv"
      },
      "source": [
        "**Aggregation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V5RWB5KXDZq"
      },
      "source": [
        "from pyspark.sql import functions as f\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/invoices.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "invoice_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"invoices.csv\"))\n",
        "\n",
        "invoice_df.show(10)\n",
        "\n",
        "invoice_df.select(f.count(\"*\").alias(\"Count *\"),\n",
        "                  f.sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
        "                  f.avg(\"UnitPrice\").alias(\"AvgPrice\"),\n",
        "                  f.countDistinct(\"InvoiceNo\").alias(\"CountDistinct\")\n",
        "                  ).show()\n",
        "\n",
        "invoice_df.selectExpr(\n",
        "                  \"count(1) as `count 1`\",\n",
        "                  \"count(StockCode) as `count field`\",\n",
        "                  \"sum(Quantity) as TotalQuantity\",\n",
        "                  \"avg(UnitPrice) as AvgPrice\"\n",
        "                ).show()\n",
        "\n",
        "invoice_df.createOrReplaceTempView(\"sales\")\n",
        "summary_sql = spark.sql(\"\"\"\n",
        "      SELECT Country, InvoiceNo,\n",
        "            sum(Quantity) as TotalQuantity,\n",
        "            round(sum(Quantity*UnitPrice),2) as InvoiceValue\n",
        "      FROM sales\n",
        "      GROUP BY Country, InvoiceNo\"\"\")\n",
        "summary_sql.show()\n",
        "\n",
        "summary_df = invoice_df \\\n",
        "    .groupBy(\"Country\", \"InvoiceNo\") \\\n",
        "    .agg(f.sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
        "         f.round(f.sum(f.expr(\"Quantity * UnitPrice\")), 2).alias(\"InvoiceValue\"),\n",
        "         f.expr(\"round(sum(Quantity * UnitPrice),2) as InvoiceValueExpr\")\n",
        "         )\n",
        "summary_df.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w5GLlrhag5r"
      },
      "source": [
        "**Group By**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDA-d543aNUz"
      },
      "source": [
        "from pyspark.sql import functions as f\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/invoices.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "invoice_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"invoices.csv\"))\n",
        "\n",
        "invoice_df.show(10)\n",
        "\n",
        "NumInvoices = f.countDistinct(\"InvoiceNo\").alias(\"NumInvoices\")\n",
        "TotalQuantity = f.sum(\"Quantity\").alias(\"TotalQuantity\")\n",
        "InvoiceValue = f.expr(\"round(sum(Quantity * UnitPrice),2) as InvoiceValue\")\n",
        "\n",
        "exSummary_df = invoice_df \\\n",
        "    .withColumn(\"InvoiceDate\", f.to_date(f.col(\"InvoiceDate\"), \"dd-MM-yyyy H.mm\")) \\\n",
        "    .where(\"year(InvoiceDate) == 2010\") \\\n",
        "    .withColumn(\"WeekNumber\", f.weekofyear(f.col(\"InvoiceDate\"))) \\\n",
        "    .groupBy(\"Country\", \"WeekNumber\") \\\n",
        "    .agg(NumInvoices, TotalQuantity, InvoiceValue)\n",
        "\n",
        "exSummary_df.sort(\"Country\", \"WeekNumber\").show()\n",
        "\n",
        "exSummary_df.coalesce(1) \\\n",
        "    .write \\\n",
        "    .format(\"parquet\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"output\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPHwGNntbjl7"
      },
      "source": [
        "!ls -l output/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBeNrPR0ccQc"
      },
      "source": [
        "**Windowing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odcF10DwcgIY"
      },
      "source": [
        "from pyspark.sql import functions as f\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/summary.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "summary_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"summary.parquet\"))\n",
        "\n",
        "summary_df.sort(\"Country\", \"WeekNumber\").show()\n",
        "\n",
        "running_total_window = Window.partitionBy(\"Country\") \\\n",
        "    .orderBy(\"WeekNumber\") \\\n",
        "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "summary_df.withColumn(\"RunningTotal\",\n",
        "                      f.sum(\"InvoiceValue\").over(running_total_window)) \\\n",
        "    .sort(\"Country\", \"WeekNumber\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSB3kYwMge77"
      },
      "source": [
        "**Ranking**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0GLi9jMgguQ"
      },
      "source": [
        "from pyspark.sql import functions as f\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/summary.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "summary_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"summary.parquet\"))\n",
        "\n",
        "summary_df.sort(\"Country\", \"WeekNumber\").show()\n",
        "\n",
        "rank_window = Window.partitionBy(\"Country\") \\\n",
        "        .orderBy(f.col(\"InvoiceValue\").desc()) \\\n",
        "        .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "df = summary_df.withColumn(\"Rank\", f.dense_rank().over(rank_window)) \\\n",
        "    .where(f.col(\"Rank\") <= 2) \\\n",
        "    .sort(\"Country\", \"Rank\") \\\n",
        "    .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPM5t3mHjEtI"
      },
      "source": [
        "**Joins**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_BFVvHBjGwE"
      },
      "source": [
        "    orders_list = [(\"01\", \"02\", 350, 1),\n",
        "                   (\"01\", \"04\", 580, 1),\n",
        "                   (\"01\", \"07\", 320, 2),\n",
        "                   (\"02\", \"03\", 450, 1),\n",
        "                   (\"02\", \"06\", 220, 1),\n",
        "                   (\"03\", \"01\", 195, 1),\n",
        "                   (\"04\", \"09\", 270, 3),\n",
        "                   (\"04\", \"08\", 410, 2),\n",
        "                   (\"05\", \"02\", 350, 1)]\n",
        "\n",
        "    order_df = spark.createDataFrame(orders_list).toDF(\"order_id\", \"prod_id\", \"unit_price\", \"qty\")\n",
        "\n",
        "    product_list = [(\"01\", \"Scroll Mouse\", 250, 20),\n",
        "                    (\"02\", \"Optical Mouse\", 350, 20),\n",
        "                    (\"03\", \"Wireless Mouse\", 450, 50),\n",
        "                    (\"04\", \"Wireless Keyboard\", 580, 50),\n",
        "                    (\"05\", \"Standard Keyboard\", 360, 10),\n",
        "                    (\"06\", \"16 GB Flash Storage\", 240, 100),\n",
        "                    (\"07\", \"32 GB Flash Storage\", 320, 50),\n",
        "                    (\"08\", \"64 GB Flash Storage\", 430, 25)]\n",
        "\n",
        "    product_df = spark.createDataFrame(product_list).toDF(\"prod_id\", \"prod_name\", \"list_price\", \"qty\")\n",
        "\n",
        "    product_df.show()\n",
        "    order_df.show()\n",
        "\n",
        "    join_expr = order_df.prod_id == product_df.prod_id\n",
        "\n",
        "    product_renamed_df = product_df.withColumnRenamed(\"qty\", \"reorder_qty\")\n",
        "\n",
        "    order_df.join(product_renamed_df, join_expr, \"inner\") \\\n",
        "        .drop(product_renamed_df.prod_id) \\\n",
        "        .select(\"order_id\", \"prod_id\", \"prod_name\", \"unit_price\", \"list_price\", \"qty\") \\\n",
        "        .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GojGKmAjUWC"
      },
      "source": [
        "    orders_list = [(\"01\", \"02\", 350, 1),\n",
        "                   (\"01\", \"04\", 580, 1),\n",
        "                   (\"01\", \"07\", 320, 2),\n",
        "                   (\"02\", \"03\", 450, 1),\n",
        "                   (\"02\", \"06\", 220, 1),\n",
        "                   (\"03\", \"01\", 195, 1),\n",
        "                   (\"04\", \"09\", 270, 3),\n",
        "                   (\"04\", \"08\", 410, 2),\n",
        "                   (\"05\", \"02\", 350, 1)]\n",
        "\n",
        "    order_df = spark.createDataFrame(orders_list).toDF(\"order_id\", \"prod_id\", \"unit_price\", \"qty\")\n",
        "\n",
        "    product_list = [(\"01\", \"Scroll Mouse\", 250, 20),\n",
        "                    (\"02\", \"Optical Mouse\", 350, 20),\n",
        "                    (\"03\", \"Wireless Mouse\", 450, 50),\n",
        "                    (\"04\", \"Wireless Keyboard\", 580, 50),\n",
        "                    (\"05\", \"Standard Keyboard\", 360, 10),\n",
        "                    (\"06\", \"16 GB Flash Storage\", 240, 100),\n",
        "                    (\"07\", \"32 GB Flash Storage\", 320, 50),\n",
        "                    (\"08\", \"64 GB Flash Storage\", 430, 25)]\n",
        "\n",
        "    product_df = spark.createDataFrame(product_list).toDF(\"prod_id\", \"prod_name\", \"list_price\", \"qty\")\n",
        "\n",
        "    product_df.show()\n",
        "    order_df.show()\n",
        "\n",
        "    join_expr = order_df.prod_id == product_df.prod_id\n",
        "\n",
        "    product_renamed_df = product_df.withColumnRenamed(\"qty\", \"reorder_qty\")\n",
        "\n",
        "    order_df.join(product_renamed_df, join_expr, \"left\") \\\n",
        "        .drop(product_renamed_df.prod_id) \\\n",
        "        .select(\"order_id\", \"prod_id\", \"prod_name\", \"unit_price\", \"list_price\", \"qty\") \\\n",
        "        .withColumn(\"prod_name\", expr(\"coalesce(prod_name, prod_id)\")) \\\n",
        "        .withColumn(\"list_price\", expr(\"coalesce(list_price, unit_price)\")) \\\n",
        "        .sort(\"order_id\") \\\n",
        "        .show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piuP7dIZXBM5"
      },
      "source": [
        "**View Log**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRtr-fRE5_3w"
      },
      "source": [
        "!cat app-logs/sparklog.log"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}