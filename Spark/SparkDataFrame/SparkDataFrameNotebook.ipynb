{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMyKD8A3HoOb384bYpi2dTk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sku1978/sk-share-repo/blob/main/Spark/SparkDataFrame/SparkDataFrameNotebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhiBxUATKbVB"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq  > /dev/null \n",
        "!wget -q https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.1.1-bin-hadoop3.2.tgz > /dev/null \n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5veJKNsaD8gv"
      },
      "source": [
        "!mkdir /content/conf /content/lib\n",
        "!wget -O /content/conf/log4j.properties https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/conf/log4j.properties > /dev/null 2>&1\n",
        "!mv /content/spark-3.1.1-bin-hadoop3.2/conf/spark-defaults.conf /content/spark-3.1.1-bin-hadoop3.2/conf/spark-defaults.conf.bk  > /dev/null 2>&1\n",
        "!wget -O /content/spark-3.1.1-bin-hadoop3.2/conf/spark-defaults.conf https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/conf/spark-defaults.conf  > /dev/null 2>&1\n",
        "!wget -O /content/conf/spark.conf https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/conf/spark.conf > /dev/null 2>&1\n",
        "\n",
        "!wget -O /content/lib/logger.py https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/lib/logger.py  > /dev/null 2>&1\n",
        "!wget -O /content/lib/utils.py https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/lib/utils.py  > /dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVA-bBFsCPyz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2540275-9cc2-4218-b6e1-35f4f4c46363"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/spark-3.1.1-bin-hadoop3.2'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fe_e2IaqXMTC"
      },
      "source": [
        "**Spark UI section**\n",
        "<br>To use Spark UI, uncomment below sections and use the public link"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhLCqy4a6ihE"
      },
      "source": [
        "#!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "#!unzip ngrok-stable-linux-amd64.zip\n",
        "#get_ipython().system_raw('./ngrok http 4050 &')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSxhuV046is1"
      },
      "source": [
        "#!curl -s http://localhost:4040/api/tunnels"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PybcmfVuWXgc"
      },
      "source": [
        "**Main Section**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jim-sE5KqDe"
      },
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark import SparkConf, SparkFiles\n",
        "from lib.logger import Log4J\n",
        "from lib.utils import get_spark_app_config\n",
        "\n",
        "conf=get_spark_app_config()\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .config(conf=conf)\\\n",
        "        .enableHiveSupport() \\\n",
        "        .getOrCreate()\n",
        "\n",
        "logger = Log4J(spark)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNbHk56QWhH0"
      },
      "source": [
        "**Basic CSV Read**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPD-t1kD5bEI"
      },
      "source": [
        "def load_csv_file(spark, url):\n",
        "   spark.sparkContext.addFile(url)\n",
        "\n",
        "   survey_df=spark.read \\\n",
        "   .format(\"csv\") \\\n",
        "   .option(\"header\", \"true\") \\\n",
        "   .option(\"inferSchema\", \"true\") \\\n",
        "   .option(\"mode\", \"FAILFAST\") \\\n",
        "   .load('file://'+SparkFiles.get(\"sample.csv\"))\n",
        "\n",
        "   return survey_df\n",
        "\n",
        "def count_by_country(survey_df):\n",
        "  count_df= survey_df.select(\"Age\", \"Gender\", \"Country\", \"State\") \\\n",
        "                     .where(\"Age <= 40\") \\\n",
        "                     .groupBy(\"Country\") \\\n",
        "                     .count()\n",
        "  return count_df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSfkRz_HWQL-"
      },
      "source": [
        "logger.info(\"Start CSV Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/sample.csv'\n",
        "\n",
        "survey_df=load_csv_file(spark, url)\n",
        "partitioned_survey_df=survey_df.repartition(2)\n",
        "\n",
        "count_df=count_by_country(partitioned_survey_df)\n",
        "\n",
        "logger.info(count_df.collect())\n",
        "\n",
        "logger.info(\"End CSV Load\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgrObFdrXOso"
      },
      "source": [
        "**Data Types**<br>\n",
        "(https://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#module-pyspark.sql.types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fy1u9idX4yC"
      },
      "source": [
        "**CSV Read (CSV)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhAtshrmYDDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc3fc43-fc4f-4dd4-850c-0a2a6f087fbc"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, StringType\n",
        "\n",
        "logger.info(\"Start Flight Time CSV Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flightSchemaStruct = StructType([StructField(\"FL_DATE\", DateType(), True),\n",
        "                                 StructField(\"OP_CARRIER\", StringType(), True),\n",
        "                                 StructField(\"OP_CARRIER_FL_NUM\", IntegerType(), True),\n",
        "                                 StructField(\"ORIGIN\", StringType(), True),\n",
        "                                 StructField(\"ORIGIN_CITY_NAME\", StringType(), True),\n",
        "                                 StructField(\"DEST\", StringType(), True),\n",
        "                                 StructField(\"DEST_CITY_NAME\", StringType(), True),\n",
        "                                 StructField(\"CRS_DEP_TIME\", IntegerType(), True),\n",
        "                                 StructField(\"DEP_TIME\", IntegerType(), True),\n",
        "                                 StructField(\"WHEELS_ON\", IntegerType(), True),\n",
        "                                 StructField(\"TAXI_IN\", IntegerType(), True),\n",
        "                                 StructField(\"CRS_ARR_TIME\", IntegerType(), True),\n",
        "                                 StructField(\"ARR_TIME\", IntegerType(), True),\n",
        "                                 StructField(\"CANCELLED\", IntegerType(), True),\n",
        "                                 StructField(\"DISTANCE\", StringType(), True),\n",
        "                                ])\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"dateFormat\", \"M/d/y\") \\\n",
        "                    .schema(flightSchemaStruct) \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.csv\"))\n",
        "\n",
        "flight_time_df.show()\n",
        "\n",
        "logger.info(\"End CSV Load\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
            "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
            "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
            "|2000-01-01|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
            "|2000-01-01|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
            "|2000-01-01|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|        2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
            "|2000-01-01|        US|             2619|   BOS|      Boston, MA| ATL|   Atlanta, GA|         650|     649|      956|      7|         955|    1003|        0|     946|\n",
            "|2000-01-01|        US|             2621|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1440|    1446|     1713|      4|        1738|    1717|        0|     946|\n",
            "|2000-01-01|        DL|              346|   BTR| Baton Rouge, LA| ATL|   Atlanta, GA|        1740|    1744|     1957|      9|        2008|    2006|        0|     449|\n",
            "|2000-01-01|        DL|              412|   BTR| Baton Rouge, LA| ATL|   Atlanta, GA|        1345|    1345|     1552|      9|        1622|    1601|        0|     449|\n",
            "|2000-01-01|        DL|              299|   BUF|     Buffalo, NY| ATL|   Atlanta, GA|        1245|    1245|     1443|      5|        1455|    1448|        0|     712|\n",
            "|2000-01-01|        DL|              495|   BUF|     Buffalo, NY| ATL|   Atlanta, GA|        2035|    2035|     2226|      9|        2241|    2235|        0|     712|\n",
            "|2000-01-01|        DL|              677|   BUF|     Buffalo, NY| ATL|   Atlanta, GA|         710|     710|      940|      7|         925|     947|        0|     712|\n",
            "|2000-01-01|        DL|              251|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        2040|    2100|     2235|      7|        2233|    2242|        0|     576|\n",
            "|2000-01-01|        DL|             1003|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1635|    1838|     2020|     12|        1832|    2032|        0|     576|\n",
            "|2000-01-01|        DL|             1501|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1430|    1435|     1623|     12|        1634|    1635|        0|     576|\n",
            "|2000-01-01|        DL|             1907|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|         530|     530|      716|      4|         723|     720|        0|     576|\n",
            "|2000-01-01|        DL|             2063|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1250|    null|     null|   null|        1449|    null|        1|     576|\n",
            "|2000-01-01|        DL|             2111|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1845|    1855|     2041|      9|        2046|    2050|        0|     576|\n",
            "|2000-01-01|        US|             2632|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|         710|     710|     null|   null|         905|    null|        0|     576|\n",
            "|2000-01-01|        US|             2967|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1700|    1700|     1845|      6|        1853|    1851|        0|     576|\n",
            "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2uWblhdVR6Q"
      },
      "source": [
        "**JSON Read**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZYy8bFcVUYP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a79026f-c994-40d9-b3d7-1dc3550f684f"
      },
      "source": [
        "logger.info(\"Start Flight Time JSON Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.json'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flightSchemaDDL = \"\"\"FL_DATE DATE, \n",
        "                     OP_CARRIER STRING, \n",
        "                     OP_CARRIER_FL_NUM INT,\n",
        "                     ORIGIN STRING,\n",
        "                     ORIGIN_CITY_NAME STRING,\n",
        "                     DEST STRING,\n",
        "                     DEST_CITY_NAME STRING,\n",
        "                     CRS_DEP_TIME INT,\n",
        "                     DEP_TIME INT,\n",
        "                     WHEELS_ON INT,\n",
        "                     TAXI_IN INT,\n",
        "                     CRS_ARR_TIME INT,\n",
        "                     ARR_TIME INT,\n",
        "                     CANCELLED INT,\n",
        "                     DISTANCE STRING\"\"\"\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"json\") \\\n",
        "                    .option(\"dateFormat\", \"M/d/y\") \\\n",
        "                    .schema(flightSchemaDDL) \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.json\"))\n",
        "\n",
        "flight_time_df.show()\n",
        "\n",
        "logger.info(\"End JSON Load\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
            "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
            "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
            "|2000-01-01|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
            "|2000-01-01|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
            "|2000-01-01|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|        2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
            "|2000-01-01|        US|             2619|   BOS|      Boston, MA| ATL|   Atlanta, GA|         650|     649|      956|      7|         955|    1003|        0|     946|\n",
            "|2000-01-01|        US|             2621|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1440|    1446|     1713|      4|        1738|    1717|        0|     946|\n",
            "|2000-01-01|        DL|              346|   BTR| Baton Rouge, LA| ATL|   Atlanta, GA|        1740|    1744|     1957|      9|        2008|    2006|        0|     449|\n",
            "|2000-01-01|        DL|              412|   BTR| Baton Rouge, LA| ATL|   Atlanta, GA|        1345|    1345|     1552|      9|        1622|    1601|        0|     449|\n",
            "|2000-01-01|        DL|              299|   BUF|     Buffalo, NY| ATL|   Atlanta, GA|        1245|    1245|     1443|      5|        1455|    1448|        0|     712|\n",
            "|2000-01-01|        DL|              495|   BUF|     Buffalo, NY| ATL|   Atlanta, GA|        2035|    2035|     2226|      9|        2241|    2235|        0|     712|\n",
            "|2000-01-01|        DL|              677|   BUF|     Buffalo, NY| ATL|   Atlanta, GA|         710|     710|      940|      7|         925|     947|        0|     712|\n",
            "|2000-01-01|        DL|              251|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        2040|    2100|     2235|      7|        2233|    2242|        0|     576|\n",
            "|2000-01-01|        DL|             1003|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1635|    1838|     2020|     12|        1832|    2032|        0|     576|\n",
            "|2000-01-01|        DL|             1501|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1430|    1435|     1623|     12|        1634|    1635|        0|     576|\n",
            "|2000-01-01|        DL|             1907|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|         530|     530|      716|      4|         723|     720|        0|     576|\n",
            "|2000-01-01|        DL|             2063|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1250|    null|     null|   null|        1449|    null|        1|     576|\n",
            "|2000-01-01|        DL|             2111|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1845|    1855|     2041|      9|        2046|    2050|        0|     576|\n",
            "|2000-01-01|        US|             2632|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|         710|     710|     null|   null|         905|    null|        0|     576|\n",
            "|2000-01-01|        US|             2967|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1700|    1700|     1845|      6|        1853|    1851|        0|     576|\n",
            "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQiEpwe1V2ii"
      },
      "source": [
        "**Parquet Read**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zW8xC5BV48M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69497c8c-f53c-451b-86ec-a76937349e3b"
      },
      "source": [
        "logger.info(\"Start Flight Time Parquet Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.parquet\"))\n",
        "\n",
        "flight_time_df.show()\n",
        "\n",
        "logger.info(\"End Parquet Load\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
            "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
            "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
            "|2000-01-01|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
            "|2000-01-01|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
            "|2000-01-01|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|        2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
            "|2000-01-01|        US|             2619|   BOS|      Boston, MA| ATL|   Atlanta, GA|         650|     649|      956|      7|         955|    1003|        0|     946|\n",
            "|2000-01-01|        US|             2621|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1440|    1446|     1713|      4|        1738|    1717|        0|     946|\n",
            "|2000-01-01|        DL|              346|   BTR| Baton Rouge, LA| ATL|   Atlanta, GA|        1740|    1744|     1957|      9|        2008|    2006|        0|     449|\n",
            "|2000-01-01|        DL|              412|   BTR| Baton Rouge, LA| ATL|   Atlanta, GA|        1345|    1345|     1552|      9|        1622|    1601|        0|     449|\n",
            "|2000-01-01|        DL|              299|   BUF|     Buffalo, NY| ATL|   Atlanta, GA|        1245|    1245|     1443|      5|        1455|    1448|        0|     712|\n",
            "|2000-01-01|        DL|              495|   BUF|     Buffalo, NY| ATL|   Atlanta, GA|        2035|    2035|     2226|      9|        2241|    2235|        0|     712|\n",
            "|2000-01-01|        DL|              677|   BUF|     Buffalo, NY| ATL|   Atlanta, GA|         710|     710|      940|      7|         925|     947|        0|     712|\n",
            "|2000-01-01|        DL|              251|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        2040|    2100|     2235|      7|        2233|    2242|        0|     576|\n",
            "|2000-01-01|        DL|             1003|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1635|    1838|     2020|     12|        1832|    2032|        0|     576|\n",
            "|2000-01-01|        DL|             1501|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1430|    1435|     1623|     12|        1634|    1635|        0|     576|\n",
            "|2000-01-01|        DL|             1907|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|         530|     530|      716|      4|         723|     720|        0|     576|\n",
            "|2000-01-01|        DL|             2063|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1250|    null|     null|   null|        1449|    null|        1|     576|\n",
            "|2000-01-01|        DL|             2111|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1845|    1855|     2041|      9|        2046|    2050|        0|     576|\n",
            "|2000-01-01|        US|             2632|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|         710|     710|     null|   null|         905|    null|        0|     576|\n",
            "|2000-01-01|        US|             2967|   BWI|   Baltimore, MD| ATL|   Atlanta, GA|        1700|    1700|     1845|      6|        1853|    1851|        0|     576|\n",
            "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPHyggQ0l_fL"
      },
      "source": [
        "**Write Avro file**<br>\n",
        "Added <br>\n",
        "spark.jars.packages                org.apache.spark:spark-avro_2.12:3.0.0\n",
        "<br>to<br>\n",
        "conf/spark-defaults.conf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwGZu8xamR5o"
      },
      "source": [
        "from pyspark.sql.functions  import spark_partition_id\n",
        "\n",
        "logger.info(\"Start Flight Time Parquet Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.parquet\"))\n",
        "\n",
        "logger.info(\"End Parquet Load\")\n",
        "logger.info(\"Number of partitions: \" + str(flight_time_df.rdd.getNumPartitions()))\n",
        "#Even though there are two partitions, since the records are less, only one partition is used. Can repartition to get more partitions in o/p\n",
        "logger.info(\"Records per partition: \" + str(flight_time_df.groupBy(spark_partition_id()).count().collect()))\n",
        "\n",
        "logger.info(\"Start Avro write\")\n",
        "\n",
        "flight_time_df.write \\\n",
        "              .format(\"avro\") \\\n",
        "              .mode(\"overwrite\") \\\n",
        "              .option(\"path\", \"dataSink/avro/\") \\\n",
        "              .save()\n",
        "\n",
        "logger.info(\"End Avro write\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pkC1hvOncFm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfee61b6-9b9f-450e-959a-4db45b930336"
      },
      "source": [
        "!ls -l dataSink/avro/"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 13088\n",
            "-rw-r--r-- 1 root root 13400431 Apr 13 06:02 part-00000-3ecfe627-485b-4fef-9771-aa4a2fb059bd-c000.avro\n",
            "-rw-r--r-- 1 root root        0 Apr 13 06:02 _SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G26doANBtA8V"
      },
      "source": [
        "**Partioned Write**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3QO6abKtFBO"
      },
      "source": [
        "logger.info(\"Start Flight Time Parquet Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.parquet\"))\n",
        "\n",
        "logger.info(\"Start Partitioned JSON write\")\n",
        "\n",
        "#To check file split at 10K records, look for this example\n",
        "#!wc -l dataSink/json/OP_CARRIER\\=DL/ORIGIN\\=ATL/*\n",
        "flight_time_df.write \\\n",
        "              .format(\"json\") \\\n",
        "              .mode(\"overwrite\") \\\n",
        "              .option(\"path\", \"dataSink/json/\") \\\n",
        "              .partitionBy(\"OP_CARRIER\", \"ORIGIN\") \\\n",
        "              .option(\"maxRecordsPerFile\", 10000) \\\n",
        "              .save()\n",
        "\n",
        "logger.info(\"End Partitioned JSON write\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCk0ylYEta56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d80b12d1-5f2d-4fe1-db64-d990775a8056"
      },
      "source": [
        "!ls -l dataSink/json/"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 40\n",
            "drwxr-xr-x  99 root root 4096 Apr 13 06:02 'OP_CARRIER=AA'\n",
            "drwxr-xr-x  36 root root 4096 Apr 13 06:02 'OP_CARRIER=AS'\n",
            "drwxr-xr-x  84 root root 4096 Apr 13 06:02 'OP_CARRIER=CO'\n",
            "drwxr-xr-x 118 root root 4096 Apr 13 06:02 'OP_CARRIER=DL'\n",
            "drwxr-xr-x  53 root root 4096 Apr 13 06:02 'OP_CARRIER=HP'\n",
            "drwxr-xr-x 119 root root 4096 Apr 13 06:02 'OP_CARRIER=NW'\n",
            "drwxr-xr-x  81 root root 4096 Apr 13 06:02 'OP_CARRIER=TW'\n",
            "drwxr-xr-x 106 root root 4096 Apr 13 06:02 'OP_CARRIER=UA'\n",
            "drwxr-xr-x  90 root root 4096 Apr 13 06:02 'OP_CARRIER=US'\n",
            "drwxr-xr-x  58 root root 4096 Apr 13 06:03 'OP_CARRIER=WN'\n",
            "-rw-r--r--   1 root root    0 Apr 13 06:03  _SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNZG1LMBmNpM"
      },
      "source": [
        "**Managed tables**<br>\n",
        "Config has been setup to write tables into *warehouse_location* folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vstXvCmGmQf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfe20e32-77d7-4b2b-a2ec-b1c613d10adc"
      },
      "source": [
        "logger.info(\"Start Flight Time Parquet Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/flight-time.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "flight_time_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"flight-time.parquet\"))\n",
        "\n",
        "logger.info(\"Create database\")\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS AIRLINES_DB\")\n",
        "spark.catalog.setCurrentDatabase(\"AIRLINES_DB\")\n",
        "\n",
        "logger.info(\"Write table\")\n",
        "flight_time_df.write \\\n",
        "              .mode(\"overwrite\") \\\n",
        "              .bucketBy(5,\"OP_CARRIER\", \"ORIGIN\") \\\n",
        "              .sortBy(\"OP_CARRIER\", \"ORIGIN\") \\\n",
        "              .saveAsTable(\"flight_data_tbl\")\n",
        "\n",
        "spark.sql(\"SELECT * FROM AIRLINES_DB.flight_data_tbl\").show(10)\n",
        "\n",
        "logger.info(spark.catalog.listTables(\"AIRLINES_DB\"))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+----------+-----------------+------+----------------+----+--------------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|      DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
            "+----------+----------+-----------------+------+----------------+----+--------------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "|2000-01-01|        AA|              369|   CMH|    Columbus, OH| DFW|Dallas/Fort Worth...|         800|     752|      920|     14|         949|     934|        0|     927|\n",
            "|2000-01-01|        AA|             1171|   CMH|    Columbus, OH| DFW|Dallas/Fort Worth...|        1850|    1843|     2025|      8|        2045|    2033|        0|     927|\n",
            "|2000-01-01|        AA|             1807|   CMH|    Columbus, OH| DFW|Dallas/Fort Worth...|        1300|    1258|     1429|      9|        1454|    1438|        0|     927|\n",
            "|2000-01-02|        AA|              369|   CMH|    Columbus, OH| DFW|Dallas/Fort Worth...|         800|     753|      935|     12|         949|     947|        0|     927|\n",
            "|2000-01-02|        AA|             1171|   CMH|    Columbus, OH| DFW|Dallas/Fort Worth...|        1850|    1847|     2028|      8|        2045|    2036|        0|     927|\n",
            "|2000-01-02|        AA|             1617|   CMH|    Columbus, OH| DFW|Dallas/Fort Worth...|         635|     632|      815|      7|         825|     822|        0|     927|\n",
            "|2000-01-02|        AA|             1807|   CMH|    Columbus, OH| DFW|Dallas/Fort Worth...|        1300|    1258|     1445|      8|        1454|    1453|        0|     927|\n",
            "|2000-01-03|        AA|              369|   CMH|    Columbus, OH| DFW|Dallas/Fort Worth...|         800|     758|      948|     11|         949|     959|        0|     927|\n",
            "|2000-01-03|        AA|             1171|   CMH|    Columbus, OH| DFW|Dallas/Fort Worth...|        1850|    1848|     2056|      6|        2045|    2102|        0|     927|\n",
            "|2000-01-03|        AA|             1617|   CMH|    Columbus, OH| DFW|Dallas/Fort Worth...|         635|     632|      835|      7|         825|     842|        0|     927|\n",
            "+----------+----------+-----------------+------+----------------+----+--------------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXZDz6YQokWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef34163d-e07a-47fa-9ec5-7552c1f00db7"
      },
      "source": [
        "!ls -lR spark-warehouse/"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "spark-warehouse/:\n",
            "total 4\n",
            "drwxr-xr-x 3 root root 4096 Apr 13 06:03 airlines_db.db\n",
            "\n",
            "spark-warehouse/airlines_db.db:\n",
            "total 4\n",
            "drwxr-xr-x 2 root root 4096 Apr 13 06:03 flight_data_tbl\n",
            "\n",
            "spark-warehouse/airlines_db.db/flight_data_tbl:\n",
            "total 3844\n",
            "-rw-r--r-- 1 root root 606771 Apr 13 06:03 part-00000-9d246f0e-3b2a-404d-87bf-7397e6105419_00000.c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root 889584 Apr 13 06:03 part-00000-9d246f0e-3b2a-404d-87bf-7397e6105419_00001.c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root 894879 Apr 13 06:03 part-00000-9d246f0e-3b2a-404d-87bf-7397e6105419_00002.c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root 867652 Apr 13 06:03 part-00000-9d246f0e-3b2a-404d-87bf-7397e6105419_00003.c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root 666858 Apr 13 06:03 part-00000-9d246f0e-3b2a-404d-87bf-7397e6105419_00004.c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root      0 Apr 13 06:03 _SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpAfKwCkM8wf"
      },
      "source": [
        "**Log File/Raw Data/Unstructured Data handling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI-GqcvjM79E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74be421d-1b4d-43fc-c312-8aa935c59276"
      },
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "logger.info(\"Start Log file load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/apache_logs.txt'\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "raw_df=spark.read \\\n",
        "                    .format(\"text\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"apache_logs.txt\"))\n",
        "\n",
        "raw_df.printSchema()\n",
        "\n",
        "log_reg = r'^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\S+) \"(\\S+)\" \"([^\"]*)'\n",
        "\n",
        "logs_df = raw_df.select(regexp_extract('value', log_reg, 1).alias('ip'),\n",
        "                        regexp_extract('value', log_reg, 4).alias('date'),\n",
        "                        regexp_extract('value', log_reg, 6).alias('request'),\n",
        "                        regexp_extract('value', log_reg, 10).alias('referrer'))\n",
        "logs_df.printSchema()\n",
        "\n",
        "logs_df.withColumn('referrer', substring_index('referrer', '/',3)) \\\n",
        "       .where(\"trim(referrer) != '-' \") \\\n",
        "       .groupBy('referrer') \\\n",
        "       .count() \\\n",
        "       .show(100, truncate=False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n",
            "root\n",
            " |-- ip: string (nullable = true)\n",
            " |-- date: string (nullable = true)\n",
            " |-- request: string (nullable = true)\n",
            " |-- referrer: string (nullable = true)\n",
            "\n",
            "+-----------------------------------+-----+\n",
            "|referrer                           |count|\n",
            "+-----------------------------------+-----+\n",
            "|http://www.semicomplete.com        |3038 |\n",
            "|http://www.google.com              |123  |\n",
            "|http://www.haskell.org             |1    |\n",
            "|http://www.google.de               |18   |\n",
            "|http://www.google.com.kh           |1    |\n",
            "|https://www.google.at              |4    |\n",
            "|http://www.google.fi               |4    |\n",
            "|https://www.google.co.in           |13   |\n",
            "|http://unix.stackexchange.com      |8    |\n",
            "|http://www.google.com.tr           |1    |\n",
            "|https://www.google.ru              |5    |\n",
            "|http://images.google.com           |1    |\n",
            "|http://avtoads.net                 |3    |\n",
            "|http://www.google.co.in            |12   |\n",
            "|https://www.google.no              |2    |\n",
            "|http://t.co                        |3    |\n",
            "|http://www.google.ee               |1    |\n",
            "|http://www.adictosalared.com       |1    |\n",
            "|http://www.google.ca               |6    |\n",
            "|https://www.google.com.br          |9    |\n",
            "|http://superuser.com               |2    |\n",
            "|https://www.google.pl              |3    |\n",
            "|http://kufli.blogspot.com          |10   |\n",
            "|http://www.google.es               |25   |\n",
            "|http://www.google.com.gt           |1    |\n",
            "|http://ijavascript.cn              |1    |\n",
            "|http://www.linux.org.ru            |4    |\n",
            "|http://www.google.ch               |1    |\n",
            "|https://www.google.ca              |3    |\n",
            "|https://www.google.cz              |5    |\n",
            "|http://www.google.com.ar           |3    |\n",
            "|http://www.bing.com                |6    |\n",
            "|http://www.google.it               |7    |\n",
            "|http://blog.andreas-haerter.com    |1    |\n",
            "|http://www.raspberrypi-spanish.es  |1    |\n",
            "|http://www.google.com.mx           |4    |\n",
            "|http://blog.float.tw               |3    |\n",
            "|http://www.linuxquestions.org      |5    |\n",
            "|https://www.google.si              |1    |\n",
            "|http://www.google.co.jp            |2    |\n",
            "|https://www.google.com.hk          |5    |\n",
            "|http://www.google.com.sg           |2    |\n",
            "|http://www.s-chassis.co.nz         |22   |\n",
            "|http://www.xiaofang.me             |1    |\n",
            "|http://www.reproductive-fitness.com|1    |\n",
            "|http://www.oschina.net             |1    |\n",
            "|https://www.google.co.za           |1    |\n",
            "|http://www.google.nl               |4    |\n",
            "|http://www.dual-boxing.com         |2    |\n",
            "|http://zoomq.qiniudn.com           |1    |\n",
            "|http://www.google.co.tz            |1    |\n",
            "|http://rungie.com                  |1    |\n",
            "|http://www.google.com.tw           |1    |\n",
            "|http://kufli.blogspot.it           |1    |\n",
            "|https://www.google.com.tr          |1    |\n",
            "|http://www.am-se.com               |3    |\n",
            "|http://tex.stackexchange.com       |1    |\n",
            "|https://www.google.ba              |3    |\n",
            "|http://znakomstvaonlain.ru         |3    |\n",
            "|https://www.google.pt              |1    |\n",
            "|http://simplestcodings.blogspot.in |1    |\n",
            "|http://www.google.lv               |2    |\n",
            "|http://www.google.be               |1    |\n",
            "|https://www.google.com.au          |2    |\n",
            "|http://www.davidsoncustom.com      |1    |\n",
            "|http://lifehacker.com              |1    |\n",
            "|http://www.google.com.mm           |1    |\n",
            "|http://zolotoy-lis.ru              |3    |\n",
            "|https://www.google.cl              |1    |\n",
            "|http://www.mqseries.net            |1    |\n",
            "|https://www.google.ee              |1    |\n",
            "|http://yandex.ru                   |2    |\n",
            "|http://community.spiceworks.com    |2    |\n",
            "|http://www.google.si               |1    |\n",
            "|http://es.wikipedia.org            |1    |\n",
            "|http://kufli.blogspot.com.au       |1    |\n",
            "|https://www.google.co.il           |2    |\n",
            "|https://www.google.com.ar          |2    |\n",
            "|http://blog.youxu.info             |1    |\n",
            "|https://www.google.hu              |1    |\n",
            "|https://encrypted.google.com       |2    |\n",
            "|http://kufli.blogspot.hu           |2    |\n",
            "|http://www.google.at               |1    |\n",
            "|http://serverfault.com             |1    |\n",
            "|http://forums.opensuse.org         |1    |\n",
            "|http://doc.ubuntu-fr.org           |1    |\n",
            "|http://www.ibm.com                 |1    |\n",
            "|https://www.google.be              |1    |\n",
            "|http://images.yandex.ru            |1    |\n",
            "|http://www.google.md               |1    |\n",
            "|http://semicomplete.com            |2001 |\n",
            "|http://www.google.ro               |1    |\n",
            "|http://en.wikipedia.org            |10   |\n",
            "|https://www.google.de              |13   |\n",
            "|http://www.google.fr               |31   |\n",
            "|http://suckless.org                |3    |\n",
            "|http://tuxradar.com                |12   |\n",
            "|https://www.google.com             |105  |\n",
            "|http://ubuntuforums.org            |4    |\n",
            "|https://www.google.co.uk           |23   |\n",
            "+-----------------------------------+-----+\n",
            "only showing top 100 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7d-BQDWVMZ7"
      },
      "source": [
        "**Handcrafted DataFrame creating Rows**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIVxtSHGVV1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553e2d59-d638-4f6b-94ae-440b60cfa9e5"
      },
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "my_schema=StructType([StructField(\"ID\", StringType(), True),\n",
        "                      StructField(\"EventDate\", StringType(), True)])\n",
        "\n",
        "my_rows=[Row(\"123\", \"21/03/2019\"), \n",
        "         Row(\"234\", \"21/05/2029\"), \n",
        "         Row(\"345\", \"01/02/2020\"),\n",
        "         Row(\"456\", \"09/12/2018\"),\n",
        "         Row(\"567\", \"03/06/2019\"),]\n",
        "\n",
        "my_rdd=spark.sparkContext.parallelize(my_rows, 2)\n",
        "\n",
        "my_df=spark.createDataFrame(my_rdd, my_schema)\n",
        "\n",
        "my_df.printSchema()\n",
        "my_df.show()\n",
        "\n",
        "new_df=my_df.withColumn('EventDate', to_date('EventDate', 'd/M/y'))\n",
        "\n",
        "new_df.printSchema()\n",
        "new_df.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- EventDate: string (nullable = true)\n",
            "\n",
            "+---+----------+\n",
            "| ID| EventDate|\n",
            "+---+----------+\n",
            "|123|21/03/2019|\n",
            "|234|21/05/2029|\n",
            "|345|01/02/2020|\n",
            "|456|09/12/2018|\n",
            "|567|03/06/2019|\n",
            "+---+----------+\n",
            "\n",
            "root\n",
            " |-- ID: string (nullable = true)\n",
            " |-- EventDate: date (nullable = true)\n",
            "\n",
            "+---+----------+\n",
            "| ID| EventDate|\n",
            "+---+----------+\n",
            "|123|2019-03-21|\n",
            "|234|2029-05-21|\n",
            "|345|2020-02-01|\n",
            "|456|2018-12-09|\n",
            "|567|2019-06-03|\n",
            "+---+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELqg2_QzKjYP"
      },
      "source": [
        "**Column Expressions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81r9iL0BKlEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccb1960e-bf31-43c8-a4c1-008b161ccc2f"
      },
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, StringType\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "logger.info(\"Start CSV Load\")\n",
        "\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/databricks-airlines.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "airlines_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .option(\"samplingRatio\", \"0.5\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"databricks-airlines.csv\"))\n",
        "\n",
        "#shows three ways to refer to columns\n",
        "airlines_df.select(\"Origin\", col(\"Dest\"), airlines_df.IsArrDelayed).show(10)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----+------------+\n",
            "|Origin|Dest|IsArrDelayed|\n",
            "+------+----+------------+\n",
            "|   SAN| SFO|         YES|\n",
            "|   SAN| SFO|         YES|\n",
            "|   SAN| SFO|         YES|\n",
            "|   SAN| SFO|          NO|\n",
            "|   SAN| SFO|         YES|\n",
            "|   SAN| SFO|          NO|\n",
            "|   SAN| SFO|         YES|\n",
            "|   SAN| SFO|         YES|\n",
            "|   SAN| SFO|         YES|\n",
            "|   SAN| SFO|         YES|\n",
            "+------+----+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71qFJoQ1OBl9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3efa5288-9d05-467d-ae8b-a617c97752b8"
      },
      "source": [
        "airlines_df.select(\"Origin\", \"Dest\", \"Distance\", \"Year\", \"Month\", \"DayOfMonth\").show(10)\n",
        "\n",
        "airlines_df.select(\"Origin\", \"Dest\", \"Distance\", expr(\"to_date(concat(Year, Month, DayOfMonth), 'yyyyMMdd') as FlightDate\")).show(10)\n",
        "\n",
        "airlines_df.select(\"Origin\", \"Dest\", \"Distance\", to_date(concat(\"Year\", \"Month\", \"DayOfMonth\"), 'yyyyMMdd').alias(\"FlightDate\")).show(10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+----+--------+----+-----+----------+\n",
            "|Origin|Dest|Distance|Year|Month|DayOfMonth|\n",
            "+------+----+--------+----+-----+----------+\n",
            "|   SAN| SFO|     447|1987|   10|        14|\n",
            "|   SAN| SFO|     447|1987|   10|        15|\n",
            "|   SAN| SFO|     447|1987|   10|        17|\n",
            "|   SAN| SFO|     447|1987|   10|        18|\n",
            "|   SAN| SFO|     447|1987|   10|        19|\n",
            "|   SAN| SFO|     447|1987|   10|        21|\n",
            "|   SAN| SFO|     447|1987|   10|        22|\n",
            "|   SAN| SFO|     447|1987|   10|        23|\n",
            "|   SAN| SFO|     447|1987|   10|        24|\n",
            "|   SAN| SFO|     447|1987|   10|        25|\n",
            "+------+----+--------+----+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+------+----+--------+----------+\n",
            "|Origin|Dest|Distance|FlightDate|\n",
            "+------+----+--------+----------+\n",
            "|   SAN| SFO|     447|1987-10-14|\n",
            "|   SAN| SFO|     447|1987-10-15|\n",
            "|   SAN| SFO|     447|1987-10-17|\n",
            "|   SAN| SFO|     447|1987-10-18|\n",
            "|   SAN| SFO|     447|1987-10-19|\n",
            "|   SAN| SFO|     447|1987-10-21|\n",
            "|   SAN| SFO|     447|1987-10-22|\n",
            "|   SAN| SFO|     447|1987-10-23|\n",
            "|   SAN| SFO|     447|1987-10-24|\n",
            "|   SAN| SFO|     447|1987-10-25|\n",
            "+------+----+--------+----------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+------+----+--------+----------+\n",
            "|Origin|Dest|Distance|FlightDate|\n",
            "+------+----+--------+----------+\n",
            "|   SAN| SFO|     447|1987-10-14|\n",
            "|   SAN| SFO|     447|1987-10-15|\n",
            "|   SAN| SFO|     447|1987-10-17|\n",
            "|   SAN| SFO|     447|1987-10-18|\n",
            "|   SAN| SFO|     447|1987-10-19|\n",
            "|   SAN| SFO|     447|1987-10-21|\n",
            "|   SAN| SFO|     447|1987-10-22|\n",
            "|   SAN| SFO|     447|1987-10-23|\n",
            "|   SAN| SFO|     447|1987-10-24|\n",
            "|   SAN| SFO|     447|1987-10-25|\n",
            "+------+----+--------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4hzYCkkWVVX"
      },
      "source": [
        "**User Defined Function (UDF)**<br>\n",
        "to be used as column expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRisIJlAXfNL"
      },
      "source": [
        "import re\n",
        "def parse_gender(gender):\n",
        "  female_pattern = r'^f$|f.m|w.m'\n",
        "  male_pattern = r'^m$|ma|m.l'\n",
        "\n",
        "  if re.search(female_pattern, gender.lower()):\n",
        "    return \"Female\"\n",
        "  elif re.search(male_pattern, gender.lower()):\n",
        "    return \"Male\"\n",
        "  else:\n",
        "    return \"Unknown\""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLhUj9sHWY6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2d3035a-0d0c-4df0-977c-9632ec0540ad"
      },
      "source": [
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/survey.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "survey_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"survey.csv\"))\n",
        "\n",
        "survey_df.show(10)\n",
        "\n",
        "#Column object UDF (no entry in catalog)\n",
        "parse_gender_udf=udf(parse_gender, StringType())\n",
        "survey_df2=survey_df.withColumn(\"Gender\", parse_gender_udf(\"Gender\"))\n",
        "survey_df2.show(10)\n",
        "\n",
        "#SQL UDF (entry goes into catalog)\n",
        "spark.udf.register(\"parse_gender_udf\", parse_gender, StringType())\n",
        "survey_df3=survey_df.withColumn(\"Gender\", expr(\"parse_gender_udf(Gender)\"))\n",
        "survey_df3.show(10)\n",
        "\n",
        "spark.catalog.listFunctions()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------------------+---+------+--------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+------------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
            "|          Timestamp|Age|Gender|       Country|state|self_employed|family_history|treatment|work_interfere|  no_employees|remote_work|tech_company|  benefits|care_options|wellness_program| seek_help| anonymity|             leave|mental_health_consequence|phys_health_consequence|   coworkers|supervisor|mental_health_interview|phys_health_interview|mental_vs_physical|obs_consequence|comments|\n",
            "+-------------------+---+------+--------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+------------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
            "|2014-08-27 11:29:31| 37|Female| United States|   IL|           NA|            No|      Yes|         Often|          6-25|         No|         Yes|       Yes|    Not sure|              No|       Yes|       Yes|     Somewhat easy|                       No|                     No|Some of them|       Yes|                     No|                Maybe|               Yes|             No|      NA|\n",
            "|2014-08-27 11:29:37| 44|     M| United States|   IN|           NA|            No|       No|        Rarely|More than 1000|         No|          No|Don't know|          No|      Don't know|Don't know|Don't know|        Don't know|                    Maybe|                     No|          No|        No|                     No|                   No|        Don't know|             No|      NA|\n",
            "|2014-08-27 11:29:44| 32|  Male|        Canada|   NA|           NA|            No|       No|        Rarely|          6-25|         No|         Yes|        No|          No|              No|        No|Don't know|Somewhat difficult|                       No|                     No|         Yes|       Yes|                    Yes|                  Yes|                No|             No|      NA|\n",
            "|2014-08-27 11:29:46| 31|  Male|United Kingdom|   NA|           NA|           Yes|      Yes|         Often|        26-100|         No|         Yes|        No|         Yes|              No|        No|        No|Somewhat difficult|                      Yes|                    Yes|Some of them|        No|                  Maybe|                Maybe|                No|            Yes|      NA|\n",
            "|2014-08-27 11:30:22| 31|  Male| United States|   TX|           NA|            No|       No|         Never|       100-500|        Yes|         Yes|       Yes|          No|      Don't know|Don't know|Don't know|        Don't know|                       No|                     No|Some of them|       Yes|                    Yes|                  Yes|        Don't know|             No|      NA|\n",
            "|2014-08-27 11:31:22| 33|  Male| United States|   TN|           NA|           Yes|       No|     Sometimes|          6-25|         No|         Yes|       Yes|    Not sure|              No|Don't know|Don't know|        Don't know|                       No|                     No|         Yes|       Yes|                     No|                Maybe|        Don't know|             No|      NA|\n",
            "|2014-08-27 11:31:50| 35|Female| United States|   MI|           NA|           Yes|      Yes|     Sometimes|           1-5|        Yes|         Yes|        No|          No|              No|        No|        No|Somewhat difficult|                    Maybe|                  Maybe|Some of them|        No|                     No|                   No|        Don't know|             No|      NA|\n",
            "|2014-08-27 11:32:05| 39|     M|        Canada|   NA|           NA|            No|       No|         Never|           1-5|        Yes|         Yes|        No|         Yes|              No|        No|       Yes|        Don't know|                       No|                     No|          No|        No|                     No|                   No|                No|             No|      NA|\n",
            "|2014-08-27 11:32:39| 42|Female| United States|   IL|           NA|           Yes|      Yes|     Sometimes|       100-500|         No|         Yes|       Yes|         Yes|              No|        No|        No|    Very difficult|                    Maybe|                     No|         Yes|       Yes|                     No|                Maybe|                No|             No|      NA|\n",
            "|2014-08-27 11:32:43| 23|  Male|        Canada|   NA|           NA|            No|       No|         Never|        26-100|         No|         Yes|Don't know|          No|      Don't know|Don't know|Don't know|        Don't know|                       No|                     No|         Yes|       Yes|                  Maybe|                Maybe|               Yes|             No|      NA|\n",
            "+-------------------+---+------+--------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+------------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+-------------------+---+------+--------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+------------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
            "|          Timestamp|Age|Gender|       Country|state|self_employed|family_history|treatment|work_interfere|  no_employees|remote_work|tech_company|  benefits|care_options|wellness_program| seek_help| anonymity|             leave|mental_health_consequence|phys_health_consequence|   coworkers|supervisor|mental_health_interview|phys_health_interview|mental_vs_physical|obs_consequence|comments|\n",
            "+-------------------+---+------+--------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+------------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
            "|2014-08-27 11:29:31| 37|Female| United States|   IL|           NA|            No|      Yes|         Often|          6-25|         No|         Yes|       Yes|    Not sure|              No|       Yes|       Yes|     Somewhat easy|                       No|                     No|Some of them|       Yes|                     No|                Maybe|               Yes|             No|      NA|\n",
            "|2014-08-27 11:29:37| 44|  Male| United States|   IN|           NA|            No|       No|        Rarely|More than 1000|         No|          No|Don't know|          No|      Don't know|Don't know|Don't know|        Don't know|                    Maybe|                     No|          No|        No|                     No|                   No|        Don't know|             No|      NA|\n",
            "|2014-08-27 11:29:44| 32|  Male|        Canada|   NA|           NA|            No|       No|        Rarely|          6-25|         No|         Yes|        No|          No|              No|        No|Don't know|Somewhat difficult|                       No|                     No|         Yes|       Yes|                    Yes|                  Yes|                No|             No|      NA|\n",
            "|2014-08-27 11:29:46| 31|  Male|United Kingdom|   NA|           NA|           Yes|      Yes|         Often|        26-100|         No|         Yes|        No|         Yes|              No|        No|        No|Somewhat difficult|                      Yes|                    Yes|Some of them|        No|                  Maybe|                Maybe|                No|            Yes|      NA|\n",
            "|2014-08-27 11:30:22| 31|  Male| United States|   TX|           NA|            No|       No|         Never|       100-500|        Yes|         Yes|       Yes|          No|      Don't know|Don't know|Don't know|        Don't know|                       No|                     No|Some of them|       Yes|                    Yes|                  Yes|        Don't know|             No|      NA|\n",
            "|2014-08-27 11:31:22| 33|  Male| United States|   TN|           NA|           Yes|       No|     Sometimes|          6-25|         No|         Yes|       Yes|    Not sure|              No|Don't know|Don't know|        Don't know|                       No|                     No|         Yes|       Yes|                     No|                Maybe|        Don't know|             No|      NA|\n",
            "|2014-08-27 11:31:50| 35|Female| United States|   MI|           NA|           Yes|      Yes|     Sometimes|           1-5|        Yes|         Yes|        No|          No|              No|        No|        No|Somewhat difficult|                    Maybe|                  Maybe|Some of them|        No|                     No|                   No|        Don't know|             No|      NA|\n",
            "|2014-08-27 11:32:05| 39|  Male|        Canada|   NA|           NA|            No|       No|         Never|           1-5|        Yes|         Yes|        No|         Yes|              No|        No|       Yes|        Don't know|                       No|                     No|          No|        No|                     No|                   No|                No|             No|      NA|\n",
            "|2014-08-27 11:32:39| 42|Female| United States|   IL|           NA|           Yes|      Yes|     Sometimes|       100-500|         No|         Yes|       Yes|         Yes|              No|        No|        No|    Very difficult|                    Maybe|                     No|         Yes|       Yes|                     No|                Maybe|                No|             No|      NA|\n",
            "|2014-08-27 11:32:43| 23|  Male|        Canada|   NA|           NA|            No|       No|         Never|        26-100|         No|         Yes|Don't know|          No|      Don't know|Don't know|Don't know|        Don't know|                       No|                     No|         Yes|       Yes|                  Maybe|                Maybe|               Yes|             No|      NA|\n",
            "+-------------------+---+------+--------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+------------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+-------------------+---+------+--------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+------------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
            "|          Timestamp|Age|Gender|       Country|state|self_employed|family_history|treatment|work_interfere|  no_employees|remote_work|tech_company|  benefits|care_options|wellness_program| seek_help| anonymity|             leave|mental_health_consequence|phys_health_consequence|   coworkers|supervisor|mental_health_interview|phys_health_interview|mental_vs_physical|obs_consequence|comments|\n",
            "+-------------------+---+------+--------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+------------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
            "|2014-08-27 11:29:31| 37|Female| United States|   IL|           NA|            No|      Yes|         Often|          6-25|         No|         Yes|       Yes|    Not sure|              No|       Yes|       Yes|     Somewhat easy|                       No|                     No|Some of them|       Yes|                     No|                Maybe|               Yes|             No|      NA|\n",
            "|2014-08-27 11:29:37| 44|  Male| United States|   IN|           NA|            No|       No|        Rarely|More than 1000|         No|          No|Don't know|          No|      Don't know|Don't know|Don't know|        Don't know|                    Maybe|                     No|          No|        No|                     No|                   No|        Don't know|             No|      NA|\n",
            "|2014-08-27 11:29:44| 32|  Male|        Canada|   NA|           NA|            No|       No|        Rarely|          6-25|         No|         Yes|        No|          No|              No|        No|Don't know|Somewhat difficult|                       No|                     No|         Yes|       Yes|                    Yes|                  Yes|                No|             No|      NA|\n",
            "|2014-08-27 11:29:46| 31|  Male|United Kingdom|   NA|           NA|           Yes|      Yes|         Often|        26-100|         No|         Yes|        No|         Yes|              No|        No|        No|Somewhat difficult|                      Yes|                    Yes|Some of them|        No|                  Maybe|                Maybe|                No|            Yes|      NA|\n",
            "|2014-08-27 11:30:22| 31|  Male| United States|   TX|           NA|            No|       No|         Never|       100-500|        Yes|         Yes|       Yes|          No|      Don't know|Don't know|Don't know|        Don't know|                       No|                     No|Some of them|       Yes|                    Yes|                  Yes|        Don't know|             No|      NA|\n",
            "|2014-08-27 11:31:22| 33|  Male| United States|   TN|           NA|           Yes|       No|     Sometimes|          6-25|         No|         Yes|       Yes|    Not sure|              No|Don't know|Don't know|        Don't know|                       No|                     No|         Yes|       Yes|                     No|                Maybe|        Don't know|             No|      NA|\n",
            "|2014-08-27 11:31:50| 35|Female| United States|   MI|           NA|           Yes|      Yes|     Sometimes|           1-5|        Yes|         Yes|        No|          No|              No|        No|        No|Somewhat difficult|                    Maybe|                  Maybe|Some of them|        No|                     No|                   No|        Don't know|             No|      NA|\n",
            "|2014-08-27 11:32:05| 39|  Male|        Canada|   NA|           NA|            No|       No|         Never|           1-5|        Yes|         Yes|        No|         Yes|              No|        No|       Yes|        Don't know|                       No|                     No|          No|        No|                     No|                   No|                No|             No|      NA|\n",
            "|2014-08-27 11:32:39| 42|Female| United States|   IL|           NA|           Yes|      Yes|     Sometimes|       100-500|         No|         Yes|       Yes|         Yes|              No|        No|        No|    Very difficult|                    Maybe|                     No|         Yes|       Yes|                     No|                Maybe|                No|             No|      NA|\n",
            "|2014-08-27 11:32:43| 23|  Male|        Canada|   NA|           NA|            No|       No|         Never|        26-100|         No|         Yes|Don't know|          No|      Don't know|Don't know|Don't know|        Don't know|                       No|                     No|         Yes|       Yes|                  Maybe|                Maybe|               Yes|             No|      NA|\n",
            "+-------------------+---+------+--------------+-----+-------------+--------------+---------+--------------+--------------+-----------+------------+----------+------------+----------------+----------+----------+------------------+-------------------------+-----------------------+------------+----------+-----------------------+---------------------+------------------+---------------+--------+\n",
            "only showing top 10 rows\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Function(name='!', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
              " Function(name='%', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
              " Function(name='&', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseAnd', isTemporary=True),\n",
              " Function(name='*', description=None, className='org.apache.spark.sql.catalyst.expressions.Multiply', isTemporary=True),\n",
              " Function(name='+', description=None, className='org.apache.spark.sql.catalyst.expressions.Add', isTemporary=True),\n",
              " Function(name='-', description=None, className='org.apache.spark.sql.catalyst.expressions.Subtract', isTemporary=True),\n",
              " Function(name='/', description=None, className='org.apache.spark.sql.catalyst.expressions.Divide', isTemporary=True),\n",
              " Function(name='<', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThan', isTemporary=True),\n",
              " Function(name='<=', description=None, className='org.apache.spark.sql.catalyst.expressions.LessThanOrEqual', isTemporary=True),\n",
              " Function(name='<=>', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualNullSafe', isTemporary=True),\n",
              " Function(name='=', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
              " Function(name='==', description=None, className='org.apache.spark.sql.catalyst.expressions.EqualTo', isTemporary=True),\n",
              " Function(name='>', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThan', isTemporary=True),\n",
              " Function(name='>=', description=None, className='org.apache.spark.sql.catalyst.expressions.GreaterThanOrEqual', isTemporary=True),\n",
              " Function(name='^', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseXor', isTemporary=True),\n",
              " Function(name='abs', description=None, className='org.apache.spark.sql.catalyst.expressions.Abs', isTemporary=True),\n",
              " Function(name='acos', description=None, className='org.apache.spark.sql.catalyst.expressions.Acos', isTemporary=True),\n",
              " Function(name='acosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Acosh', isTemporary=True),\n",
              " Function(name='add_months', description=None, className='org.apache.spark.sql.catalyst.expressions.AddMonths', isTemporary=True),\n",
              " Function(name='aggregate', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayAggregate', isTemporary=True),\n",
              " Function(name='and', description=None, className='org.apache.spark.sql.catalyst.expressions.And', isTemporary=True),\n",
              " Function(name='any', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
              " Function(name='approx_count_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.HyperLogLogPlusPlus', isTemporary=True),\n",
              " Function(name='approx_percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
              " Function(name='array', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateArray', isTemporary=True),\n",
              " Function(name='array_contains', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayContains', isTemporary=True),\n",
              " Function(name='array_distinct', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayDistinct', isTemporary=True),\n",
              " Function(name='array_except', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExcept', isTemporary=True),\n",
              " Function(name='array_intersect', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayIntersect', isTemporary=True),\n",
              " Function(name='array_join', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayJoin', isTemporary=True),\n",
              " Function(name='array_max', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMax', isTemporary=True),\n",
              " Function(name='array_min', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayMin', isTemporary=True),\n",
              " Function(name='array_position', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayPosition', isTemporary=True),\n",
              " Function(name='array_remove', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRemove', isTemporary=True),\n",
              " Function(name='array_repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayRepeat', isTemporary=True),\n",
              " Function(name='array_sort', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraySort', isTemporary=True),\n",
              " Function(name='array_union', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayUnion', isTemporary=True),\n",
              " Function(name='arrays_overlap', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysOverlap', isTemporary=True),\n",
              " Function(name='arrays_zip', description=None, className='org.apache.spark.sql.catalyst.expressions.ArraysZip', isTemporary=True),\n",
              " Function(name='ascii', description=None, className='org.apache.spark.sql.catalyst.expressions.Ascii', isTemporary=True),\n",
              " Function(name='asin', description=None, className='org.apache.spark.sql.catalyst.expressions.Asin', isTemporary=True),\n",
              " Function(name='asinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Asinh', isTemporary=True),\n",
              " Function(name='assert_true', description=None, className='org.apache.spark.sql.catalyst.expressions.AssertTrue', isTemporary=True),\n",
              " Function(name='atan', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan', isTemporary=True),\n",
              " Function(name='atan2', description=None, className='org.apache.spark.sql.catalyst.expressions.Atan2', isTemporary=True),\n",
              " Function(name='atanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Atanh', isTemporary=True),\n",
              " Function(name='avg', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
              " Function(name='base64', description=None, className='org.apache.spark.sql.catalyst.expressions.Base64', isTemporary=True),\n",
              " Function(name='bigint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='bin', description=None, className='org.apache.spark.sql.catalyst.expressions.Bin', isTemporary=True),\n",
              " Function(name='binary', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='bit_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitAndAgg', isTemporary=True),\n",
              " Function(name='bit_count', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseCount', isTemporary=True),\n",
              " Function(name='bit_length', description=None, className='org.apache.spark.sql.catalyst.expressions.BitLength', isTemporary=True),\n",
              " Function(name='bit_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitOrAgg', isTemporary=True),\n",
              " Function(name='bit_xor', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BitXorAgg', isTemporary=True),\n",
              " Function(name='bool_and', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
              " Function(name='bool_or', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
              " Function(name='boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='bround', description=None, className='org.apache.spark.sql.catalyst.expressions.BRound', isTemporary=True),\n",
              " Function(name='cardinality', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
              " Function(name='cast', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='cbrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Cbrt', isTemporary=True),\n",
              " Function(name='ceil', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
              " Function(name='ceiling', description=None, className='org.apache.spark.sql.catalyst.expressions.Ceil', isTemporary=True),\n",
              " Function(name='char', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
              " Function(name='char_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
              " Function(name='character_length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
              " Function(name='chr', description=None, className='org.apache.spark.sql.catalyst.expressions.Chr', isTemporary=True),\n",
              " Function(name='coalesce', description=None, className='org.apache.spark.sql.catalyst.expressions.Coalesce', isTemporary=True),\n",
              " Function(name='collect_list', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectList', isTemporary=True),\n",
              " Function(name='collect_set', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CollectSet', isTemporary=True),\n",
              " Function(name='concat', description=None, className='org.apache.spark.sql.catalyst.expressions.Concat', isTemporary=True),\n",
              " Function(name='concat_ws', description=None, className='org.apache.spark.sql.catalyst.expressions.ConcatWs', isTemporary=True),\n",
              " Function(name='conv', description=None, className='org.apache.spark.sql.catalyst.expressions.Conv', isTemporary=True),\n",
              " Function(name='corr', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Corr', isTemporary=True),\n",
              " Function(name='cos', description=None, className='org.apache.spark.sql.catalyst.expressions.Cos', isTemporary=True),\n",
              " Function(name='cosh', description=None, className='org.apache.spark.sql.catalyst.expressions.Cosh', isTemporary=True),\n",
              " Function(name='cot', description=None, className='org.apache.spark.sql.catalyst.expressions.Cot', isTemporary=True),\n",
              " Function(name='count', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Count', isTemporary=True),\n",
              " Function(name='count_if', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountIf', isTemporary=True),\n",
              " Function(name='count_min_sketch', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CountMinSketchAgg', isTemporary=True),\n",
              " Function(name='covar_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovPopulation', isTemporary=True),\n",
              " Function(name='covar_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.CovSample', isTemporary=True),\n",
              " Function(name='crc32', description=None, className='org.apache.spark.sql.catalyst.expressions.Crc32', isTemporary=True),\n",
              " Function(name='cube', description=None, className='org.apache.spark.sql.catalyst.expressions.Cube', isTemporary=True),\n",
              " Function(name='cume_dist', description=None, className='org.apache.spark.sql.catalyst.expressions.CumeDist', isTemporary=True),\n",
              " Function(name='current_catalog', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentCatalog', isTemporary=True),\n",
              " Function(name='current_database', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDatabase', isTemporary=True),\n",
              " Function(name='current_date', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentDate', isTemporary=True),\n",
              " Function(name='current_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimestamp', isTemporary=True),\n",
              " Function(name='current_timezone', description=None, className='org.apache.spark.sql.catalyst.expressions.CurrentTimeZone', isTemporary=True),\n",
              " Function(name='date', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='date_add', description=None, className='org.apache.spark.sql.catalyst.expressions.DateAdd', isTemporary=True),\n",
              " Function(name='date_format', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFormatClass', isTemporary=True),\n",
              " Function(name='date_from_unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.DateFromUnixDate', isTemporary=True),\n",
              " Function(name='date_part', description=None, className='org.apache.spark.sql.catalyst.expressions.DatePart', isTemporary=True),\n",
              " Function(name='date_sub', description=None, className='org.apache.spark.sql.catalyst.expressions.DateSub', isTemporary=True),\n",
              " Function(name='date_trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncTimestamp', isTemporary=True),\n",
              " Function(name='datediff', description=None, className='org.apache.spark.sql.catalyst.expressions.DateDiff', isTemporary=True),\n",
              " Function(name='day', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
              " Function(name='dayofmonth', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfMonth', isTemporary=True),\n",
              " Function(name='dayofweek', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfWeek', isTemporary=True),\n",
              " Function(name='dayofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.DayOfYear', isTemporary=True),\n",
              " Function(name='decimal', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='decode', description=None, className='org.apache.spark.sql.catalyst.expressions.Decode', isTemporary=True),\n",
              " Function(name='degrees', description=None, className='org.apache.spark.sql.catalyst.expressions.ToDegrees', isTemporary=True),\n",
              " Function(name='dense_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.DenseRank', isTemporary=True),\n",
              " Function(name='div', description=None, className='org.apache.spark.sql.catalyst.expressions.IntegralDivide', isTemporary=True),\n",
              " Function(name='double', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='e', description=None, className='org.apache.spark.sql.catalyst.expressions.EulerNumber', isTemporary=True),\n",
              " Function(name='element_at', description=None, className='org.apache.spark.sql.catalyst.expressions.ElementAt', isTemporary=True),\n",
              " Function(name='elt', description=None, className='org.apache.spark.sql.catalyst.expressions.Elt', isTemporary=True),\n",
              " Function(name='encode', description=None, className='org.apache.spark.sql.catalyst.expressions.Encode', isTemporary=True),\n",
              " Function(name='every', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolAnd', isTemporary=True),\n",
              " Function(name='exists', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayExists', isTemporary=True),\n",
              " Function(name='exp', description=None, className='org.apache.spark.sql.catalyst.expressions.Exp', isTemporary=True),\n",
              " Function(name='explode', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
              " Function(name='explode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Explode', isTemporary=True),\n",
              " Function(name='expm1', description=None, className='org.apache.spark.sql.catalyst.expressions.Expm1', isTemporary=True),\n",
              " Function(name='extract', description=None, className='org.apache.spark.sql.catalyst.expressions.Extract', isTemporary=True),\n",
              " Function(name='factorial', description=None, className='org.apache.spark.sql.catalyst.expressions.Factorial', isTemporary=True),\n",
              " Function(name='filter', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayFilter', isTemporary=True),\n",
              " Function(name='find_in_set', description=None, className='org.apache.spark.sql.catalyst.expressions.FindInSet', isTemporary=True),\n",
              " Function(name='first', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
              " Function(name='first_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.First', isTemporary=True),\n",
              " Function(name='flatten', description=None, className='org.apache.spark.sql.catalyst.expressions.Flatten', isTemporary=True),\n",
              " Function(name='float', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='floor', description=None, className='org.apache.spark.sql.catalyst.expressions.Floor', isTemporary=True),\n",
              " Function(name='forall', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayForAll', isTemporary=True),\n",
              " Function(name='format_number', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatNumber', isTemporary=True),\n",
              " Function(name='format_string', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
              " Function(name='from_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.CsvToStructs', isTemporary=True),\n",
              " Function(name='from_json', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonToStructs', isTemporary=True),\n",
              " Function(name='from_unixtime', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUnixTime', isTemporary=True),\n",
              " Function(name='from_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.FromUTCTimestamp', isTemporary=True),\n",
              " Function(name='get_json_object', description=None, className='org.apache.spark.sql.catalyst.expressions.GetJsonObject', isTemporary=True),\n",
              " Function(name='greatest', description=None, className='org.apache.spark.sql.catalyst.expressions.Greatest', isTemporary=True),\n",
              " Function(name='grouping', description=None, className='org.apache.spark.sql.catalyst.expressions.Grouping', isTemporary=True),\n",
              " Function(name='grouping_id', description=None, className='org.apache.spark.sql.catalyst.expressions.GroupingID', isTemporary=True),\n",
              " Function(name='hash', description=None, className='org.apache.spark.sql.catalyst.expressions.Murmur3Hash', isTemporary=True),\n",
              " Function(name='hex', description=None, className='org.apache.spark.sql.catalyst.expressions.Hex', isTemporary=True),\n",
              " Function(name='hour', description=None, className='org.apache.spark.sql.catalyst.expressions.Hour', isTemporary=True),\n",
              " Function(name='hypot', description=None, className='org.apache.spark.sql.catalyst.expressions.Hypot', isTemporary=True),\n",
              " Function(name='if', description=None, className='org.apache.spark.sql.catalyst.expressions.If', isTemporary=True),\n",
              " Function(name='ifnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IfNull', isTemporary=True),\n",
              " Function(name='in', description=None, className='org.apache.spark.sql.catalyst.expressions.In', isTemporary=True),\n",
              " Function(name='initcap', description=None, className='org.apache.spark.sql.catalyst.expressions.InitCap', isTemporary=True),\n",
              " Function(name='inline', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
              " Function(name='inline_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.Inline', isTemporary=True),\n",
              " Function(name='input_file_block_length', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockLength', isTemporary=True),\n",
              " Function(name='input_file_block_start', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileBlockStart', isTemporary=True),\n",
              " Function(name='input_file_name', description=None, className='org.apache.spark.sql.catalyst.expressions.InputFileName', isTemporary=True),\n",
              " Function(name='instr', description=None, className='org.apache.spark.sql.catalyst.expressions.StringInstr', isTemporary=True),\n",
              " Function(name='int', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='isnan', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNaN', isTemporary=True),\n",
              " Function(name='isnotnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNotNull', isTemporary=True),\n",
              " Function(name='isnull', description=None, className='org.apache.spark.sql.catalyst.expressions.IsNull', isTemporary=True),\n",
              " Function(name='java_method', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
              " Function(name='json_array_length', description=None, className='org.apache.spark.sql.catalyst.expressions.LengthOfJsonArray', isTemporary=True),\n",
              " Function(name='json_object_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonObjectKeys', isTemporary=True),\n",
              " Function(name='json_tuple', description=None, className='org.apache.spark.sql.catalyst.expressions.JsonTuple', isTemporary=True),\n",
              " Function(name='kurtosis', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Kurtosis', isTemporary=True),\n",
              " Function(name='lag', description=None, className='org.apache.spark.sql.catalyst.expressions.Lag', isTemporary=True),\n",
              " Function(name='last', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
              " Function(name='last_day', description=None, className='org.apache.spark.sql.catalyst.expressions.LastDay', isTemporary=True),\n",
              " Function(name='last_value', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Last', isTemporary=True),\n",
              " Function(name='lcase', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
              " Function(name='lead', description=None, className='org.apache.spark.sql.catalyst.expressions.Lead', isTemporary=True),\n",
              " Function(name='least', description=None, className='org.apache.spark.sql.catalyst.expressions.Least', isTemporary=True),\n",
              " Function(name='left', description=None, className='org.apache.spark.sql.catalyst.expressions.Left', isTemporary=True),\n",
              " Function(name='length', description=None, className='org.apache.spark.sql.catalyst.expressions.Length', isTemporary=True),\n",
              " Function(name='levenshtein', description=None, className='org.apache.spark.sql.catalyst.expressions.Levenshtein', isTemporary=True),\n",
              " Function(name='like', description=None, className='org.apache.spark.sql.catalyst.expressions.Like', isTemporary=True),\n",
              " Function(name='ln', description=None, className='org.apache.spark.sql.catalyst.expressions.Log', isTemporary=True),\n",
              " Function(name='locate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
              " Function(name='log', description=None, className='org.apache.spark.sql.catalyst.expressions.Logarithm', isTemporary=True),\n",
              " Function(name='log10', description=None, className='org.apache.spark.sql.catalyst.expressions.Log10', isTemporary=True),\n",
              " Function(name='log1p', description=None, className='org.apache.spark.sql.catalyst.expressions.Log1p', isTemporary=True),\n",
              " Function(name='log2', description=None, className='org.apache.spark.sql.catalyst.expressions.Log2', isTemporary=True),\n",
              " Function(name='lower', description=None, className='org.apache.spark.sql.catalyst.expressions.Lower', isTemporary=True),\n",
              " Function(name='lpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLPad', isTemporary=True),\n",
              " Function(name='ltrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimLeft', isTemporary=True),\n",
              " Function(name='make_date', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeDate', isTemporary=True),\n",
              " Function(name='make_interval', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeInterval', isTemporary=True),\n",
              " Function(name='make_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.MakeTimestamp', isTemporary=True),\n",
              " Function(name='map', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateMap', isTemporary=True),\n",
              " Function(name='map_concat', description=None, className='org.apache.spark.sql.catalyst.expressions.MapConcat', isTemporary=True),\n",
              " Function(name='map_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapEntries', isTemporary=True),\n",
              " Function(name='map_filter', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFilter', isTemporary=True),\n",
              " Function(name='map_from_arrays', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromArrays', isTemporary=True),\n",
              " Function(name='map_from_entries', description=None, className='org.apache.spark.sql.catalyst.expressions.MapFromEntries', isTemporary=True),\n",
              " Function(name='map_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.MapKeys', isTemporary=True),\n",
              " Function(name='map_values', description=None, className='org.apache.spark.sql.catalyst.expressions.MapValues', isTemporary=True),\n",
              " Function(name='map_zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.MapZipWith', isTemporary=True),\n",
              " Function(name='max', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Max', isTemporary=True),\n",
              " Function(name='max_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MaxBy', isTemporary=True),\n",
              " Function(name='md5', description=None, className='org.apache.spark.sql.catalyst.expressions.Md5', isTemporary=True),\n",
              " Function(name='mean', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Average', isTemporary=True),\n",
              " Function(name='min', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Min', isTemporary=True),\n",
              " Function(name='min_by', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.MinBy', isTemporary=True),\n",
              " Function(name='minute', description=None, className='org.apache.spark.sql.catalyst.expressions.Minute', isTemporary=True),\n",
              " Function(name='mod', description=None, className='org.apache.spark.sql.catalyst.expressions.Remainder', isTemporary=True),\n",
              " Function(name='monotonically_increasing_id', description=None, className='org.apache.spark.sql.catalyst.expressions.MonotonicallyIncreasingID', isTemporary=True),\n",
              " Function(name='month', description=None, className='org.apache.spark.sql.catalyst.expressions.Month', isTemporary=True),\n",
              " Function(name='months_between', description=None, className='org.apache.spark.sql.catalyst.expressions.MonthsBetween', isTemporary=True),\n",
              " Function(name='named_struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
              " Function(name='nanvl', description=None, className='org.apache.spark.sql.catalyst.expressions.NaNvl', isTemporary=True),\n",
              " Function(name='negative', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryMinus', isTemporary=True),\n",
              " Function(name='next_day', description=None, className='org.apache.spark.sql.catalyst.expressions.NextDay', isTemporary=True),\n",
              " Function(name='not', description=None, className='org.apache.spark.sql.catalyst.expressions.Not', isTemporary=True),\n",
              " Function(name='now', description=None, className='org.apache.spark.sql.catalyst.expressions.Now', isTemporary=True),\n",
              " Function(name='nth_value', description=None, className='org.apache.spark.sql.catalyst.expressions.NthValue', isTemporary=True),\n",
              " Function(name='ntile', description=None, className='org.apache.spark.sql.catalyst.expressions.NTile', isTemporary=True),\n",
              " Function(name='nullif', description=None, className='org.apache.spark.sql.catalyst.expressions.NullIf', isTemporary=True),\n",
              " Function(name='nvl', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl', isTemporary=True),\n",
              " Function(name='nvl2', description=None, className='org.apache.spark.sql.catalyst.expressions.Nvl2', isTemporary=True),\n",
              " Function(name='octet_length', description=None, className='org.apache.spark.sql.catalyst.expressions.OctetLength', isTemporary=True),\n",
              " Function(name='or', description=None, className='org.apache.spark.sql.catalyst.expressions.Or', isTemporary=True),\n",
              " Function(name='overlay', description=None, className='org.apache.spark.sql.catalyst.expressions.Overlay', isTemporary=True),\n",
              " Function(name='parse_gender_udf', description=None, className='org.apache.spark.sql.UDFRegistration$$Lambda$3918/1890565604', isTemporary=True),\n",
              " Function(name='parse_url', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseUrl', isTemporary=True),\n",
              " Function(name='percent_rank', description=None, className='org.apache.spark.sql.catalyst.expressions.PercentRank', isTemporary=True),\n",
              " Function(name='percentile', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Percentile', isTemporary=True),\n",
              " Function(name='percentile_approx', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile', isTemporary=True),\n",
              " Function(name='pi', description=None, className='org.apache.spark.sql.catalyst.expressions.Pi', isTemporary=True),\n",
              " Function(name='pmod', description=None, className='org.apache.spark.sql.catalyst.expressions.Pmod', isTemporary=True),\n",
              " Function(name='posexplode', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
              " Function(name='posexplode_outer', description=None, className='org.apache.spark.sql.catalyst.expressions.PosExplode', isTemporary=True),\n",
              " Function(name='position', description=None, className='org.apache.spark.sql.catalyst.expressions.StringLocate', isTemporary=True),\n",
              " Function(name='positive', description=None, className='org.apache.spark.sql.catalyst.expressions.UnaryPositive', isTemporary=True),\n",
              " Function(name='pow', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
              " Function(name='power', description=None, className='org.apache.spark.sql.catalyst.expressions.Pow', isTemporary=True),\n",
              " Function(name='printf', description=None, className='org.apache.spark.sql.catalyst.expressions.FormatString', isTemporary=True),\n",
              " Function(name='quarter', description=None, className='org.apache.spark.sql.catalyst.expressions.Quarter', isTemporary=True),\n",
              " Function(name='radians', description=None, className='org.apache.spark.sql.catalyst.expressions.ToRadians', isTemporary=True),\n",
              " Function(name='raise_error', description=None, className='org.apache.spark.sql.catalyst.expressions.RaiseError', isTemporary=True),\n",
              " Function(name='rand', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
              " Function(name='randn', description=None, className='org.apache.spark.sql.catalyst.expressions.Randn', isTemporary=True),\n",
              " Function(name='random', description=None, className='org.apache.spark.sql.catalyst.expressions.Rand', isTemporary=True),\n",
              " Function(name='rank', description=None, className='org.apache.spark.sql.catalyst.expressions.Rank', isTemporary=True),\n",
              " Function(name='reflect', description=None, className='org.apache.spark.sql.catalyst.expressions.CallMethodViaReflection', isTemporary=True),\n",
              " Function(name='regexp_extract', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtract', isTemporary=True),\n",
              " Function(name='regexp_extract_all', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpExtractAll', isTemporary=True),\n",
              " Function(name='regexp_replace', description=None, className='org.apache.spark.sql.catalyst.expressions.RegExpReplace', isTemporary=True),\n",
              " Function(name='repeat', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRepeat', isTemporary=True),\n",
              " Function(name='replace', description=None, className='org.apache.spark.sql.catalyst.expressions.StringReplace', isTemporary=True),\n",
              " Function(name='reverse', description=None, className='org.apache.spark.sql.catalyst.expressions.Reverse', isTemporary=True),\n",
              " Function(name='right', description=None, className='org.apache.spark.sql.catalyst.expressions.Right', isTemporary=True),\n",
              " Function(name='rint', description=None, className='org.apache.spark.sql.catalyst.expressions.Rint', isTemporary=True),\n",
              " Function(name='rlike', description=None, className='org.apache.spark.sql.catalyst.expressions.RLike', isTemporary=True),\n",
              " Function(name='rollup', description=None, className='org.apache.spark.sql.catalyst.expressions.Rollup', isTemporary=True),\n",
              " Function(name='round', description=None, className='org.apache.spark.sql.catalyst.expressions.Round', isTemporary=True),\n",
              " Function(name='row_number', description=None, className='org.apache.spark.sql.catalyst.expressions.RowNumber', isTemporary=True),\n",
              " Function(name='rpad', description=None, className='org.apache.spark.sql.catalyst.expressions.StringRPad', isTemporary=True),\n",
              " Function(name='rtrim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrimRight', isTemporary=True),\n",
              " Function(name='schema_of_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfCsv', isTemporary=True),\n",
              " Function(name='schema_of_json', description=None, className='org.apache.spark.sql.catalyst.expressions.SchemaOfJson', isTemporary=True),\n",
              " Function(name='second', description=None, className='org.apache.spark.sql.catalyst.expressions.Second', isTemporary=True),\n",
              " Function(name='sentences', description=None, className='org.apache.spark.sql.catalyst.expressions.Sentences', isTemporary=True),\n",
              " Function(name='sequence', description=None, className='org.apache.spark.sql.catalyst.expressions.Sequence', isTemporary=True),\n",
              " Function(name='sha', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
              " Function(name='sha1', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha1', isTemporary=True),\n",
              " Function(name='sha2', description=None, className='org.apache.spark.sql.catalyst.expressions.Sha2', isTemporary=True),\n",
              " Function(name='shiftleft', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftLeft', isTemporary=True),\n",
              " Function(name='shiftright', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRight', isTemporary=True),\n",
              " Function(name='shiftrightunsigned', description=None, className='org.apache.spark.sql.catalyst.expressions.ShiftRightUnsigned', isTemporary=True),\n",
              " Function(name='shuffle', description=None, className='org.apache.spark.sql.catalyst.expressions.Shuffle', isTemporary=True),\n",
              " Function(name='sign', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
              " Function(name='signum', description=None, className='org.apache.spark.sql.catalyst.expressions.Signum', isTemporary=True),\n",
              " Function(name='sin', description=None, className='org.apache.spark.sql.catalyst.expressions.Sin', isTemporary=True),\n",
              " Function(name='sinh', description=None, className='org.apache.spark.sql.catalyst.expressions.Sinh', isTemporary=True),\n",
              " Function(name='size', description=None, className='org.apache.spark.sql.catalyst.expressions.Size', isTemporary=True),\n",
              " Function(name='skewness', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Skewness', isTemporary=True),\n",
              " Function(name='slice', description=None, className='org.apache.spark.sql.catalyst.expressions.Slice', isTemporary=True),\n",
              " Function(name='smallint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='some', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.BoolOr', isTemporary=True),\n",
              " Function(name='sort_array', description=None, className='org.apache.spark.sql.catalyst.expressions.SortArray', isTemporary=True),\n",
              " Function(name='soundex', description=None, className='org.apache.spark.sql.catalyst.expressions.SoundEx', isTemporary=True),\n",
              " Function(name='space', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSpace', isTemporary=True),\n",
              " Function(name='spark_partition_id', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkPartitionID', isTemporary=True),\n",
              " Function(name='split', description=None, className='org.apache.spark.sql.catalyst.expressions.StringSplit', isTemporary=True),\n",
              " Function(name='sqrt', description=None, className='org.apache.spark.sql.catalyst.expressions.Sqrt', isTemporary=True),\n",
              " Function(name='stack', description=None, className='org.apache.spark.sql.catalyst.expressions.Stack', isTemporary=True),\n",
              " Function(name='std', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
              " Function(name='stddev', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
              " Function(name='stddev_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevPop', isTemporary=True),\n",
              " Function(name='stddev_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.StddevSamp', isTemporary=True),\n",
              " Function(name='str_to_map', description=None, className='org.apache.spark.sql.catalyst.expressions.StringToMap', isTemporary=True),\n",
              " Function(name='string', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='struct', description=None, className='org.apache.spark.sql.catalyst.expressions.CreateNamedStruct', isTemporary=True),\n",
              " Function(name='substr', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
              " Function(name='substring', description=None, className='org.apache.spark.sql.catalyst.expressions.Substring', isTemporary=True),\n",
              " Function(name='substring_index', description=None, className='org.apache.spark.sql.catalyst.expressions.SubstringIndex', isTemporary=True),\n",
              " Function(name='sum', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.Sum', isTemporary=True),\n",
              " Function(name='tan', description=None, className='org.apache.spark.sql.catalyst.expressions.Tan', isTemporary=True),\n",
              " Function(name='tanh', description=None, className='org.apache.spark.sql.catalyst.expressions.Tanh', isTemporary=True),\n",
              " Function(name='timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='timestamp_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.MicrosToTimestamp', isTemporary=True),\n",
              " Function(name='timestamp_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.MillisToTimestamp', isTemporary=True),\n",
              " Function(name='timestamp_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.SecondsToTimestamp', isTemporary=True),\n",
              " Function(name='tinyint', description=None, className='org.apache.spark.sql.catalyst.expressions.Cast', isTemporary=True),\n",
              " Function(name='to_csv', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToCsv', isTemporary=True),\n",
              " Function(name='to_date', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToDate', isTemporary=True),\n",
              " Function(name='to_json', description=None, className='org.apache.spark.sql.catalyst.expressions.StructsToJson', isTemporary=True),\n",
              " Function(name='to_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ParseToTimestamp', isTemporary=True),\n",
              " Function(name='to_unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUnixTimestamp', isTemporary=True),\n",
              " Function(name='to_utc_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.ToUTCTimestamp', isTemporary=True),\n",
              " Function(name='transform', description=None, className='org.apache.spark.sql.catalyst.expressions.ArrayTransform', isTemporary=True),\n",
              " Function(name='transform_keys', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformKeys', isTemporary=True),\n",
              " Function(name='transform_values', description=None, className='org.apache.spark.sql.catalyst.expressions.TransformValues', isTemporary=True),\n",
              " Function(name='translate', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTranslate', isTemporary=True),\n",
              " Function(name='trim', description=None, className='org.apache.spark.sql.catalyst.expressions.StringTrim', isTemporary=True),\n",
              " Function(name='trunc', description=None, className='org.apache.spark.sql.catalyst.expressions.TruncDate', isTemporary=True),\n",
              " Function(name='typeof', description=None, className='org.apache.spark.sql.catalyst.expressions.TypeOf', isTemporary=True),\n",
              " Function(name='ucase', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
              " Function(name='unbase64', description=None, className='org.apache.spark.sql.catalyst.expressions.UnBase64', isTemporary=True),\n",
              " Function(name='unhex', description=None, className='org.apache.spark.sql.catalyst.expressions.Unhex', isTemporary=True),\n",
              " Function(name='unix_date', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixDate', isTemporary=True),\n",
              " Function(name='unix_micros', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMicros', isTemporary=True),\n",
              " Function(name='unix_millis', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixMillis', isTemporary=True),\n",
              " Function(name='unix_seconds', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixSeconds', isTemporary=True),\n",
              " Function(name='unix_timestamp', description=None, className='org.apache.spark.sql.catalyst.expressions.UnixTimestamp', isTemporary=True),\n",
              " Function(name='upper', description=None, className='org.apache.spark.sql.catalyst.expressions.Upper', isTemporary=True),\n",
              " Function(name='uuid', description=None, className='org.apache.spark.sql.catalyst.expressions.Uuid', isTemporary=True),\n",
              " Function(name='var_pop', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VariancePop', isTemporary=True),\n",
              " Function(name='var_samp', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
              " Function(name='variance', description=None, className='org.apache.spark.sql.catalyst.expressions.aggregate.VarianceSamp', isTemporary=True),\n",
              " Function(name='version', description=None, className='org.apache.spark.sql.catalyst.expressions.SparkVersion', isTemporary=True),\n",
              " Function(name='weekday', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekDay', isTemporary=True),\n",
              " Function(name='weekofyear', description=None, className='org.apache.spark.sql.catalyst.expressions.WeekOfYear', isTemporary=True),\n",
              " Function(name='when', description=None, className='org.apache.spark.sql.catalyst.expressions.CaseWhen', isTemporary=True),\n",
              " Function(name='width_bucket', description=None, className='org.apache.spark.sql.catalyst.expressions.WidthBucket', isTemporary=True),\n",
              " Function(name='window', description=None, className='org.apache.spark.sql.catalyst.expressions.TimeWindow', isTemporary=True),\n",
              " Function(name='xpath', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathList', isTemporary=True),\n",
              " Function(name='xpath_boolean', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathBoolean', isTemporary=True),\n",
              " Function(name='xpath_double', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
              " Function(name='xpath_float', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathFloat', isTemporary=True),\n",
              " Function(name='xpath_int', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathInt', isTemporary=True),\n",
              " Function(name='xpath_long', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathLong', isTemporary=True),\n",
              " Function(name='xpath_number', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathDouble', isTemporary=True),\n",
              " Function(name='xpath_short', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathShort', isTemporary=True),\n",
              " Function(name='xpath_string', description=None, className='org.apache.spark.sql.catalyst.expressions.xml.XPathString', isTemporary=True),\n",
              " Function(name='xxhash64', description=None, className='org.apache.spark.sql.catalyst.expressions.XxHash64', isTemporary=True),\n",
              " Function(name='year', description=None, className='org.apache.spark.sql.catalyst.expressions.Year', isTemporary=True),\n",
              " Function(name='zip_with', description=None, className='org.apache.spark.sql.catalyst.expressions.ZipWith', isTemporary=True),\n",
              " Function(name='|', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseOr', isTemporary=True),\n",
              " Function(name='~', description=None, className='org.apache.spark.sql.catalyst.expressions.BitwiseNot', isTemporary=True)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNT0rnKxeFt2"
      },
      "source": [
        "**Miscellaneous functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwRb8OEFeM-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db0b3af5-d927-4506-e7a9-706be91c7dcb"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, monotonically_increasing_id, when, expr\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "data_list = [(\"Ravi\", \"28\", \"1\", \"2002\"),\n",
        "             (\"Abdul\", \"23\", \"5\", \"81\"),  # 1981\n",
        "             (\"John\", \"12\", \"12\", \"6\"),  # 2006\n",
        "             (\"Rosy\", \"7\", \"8\", \"63\"),  # 1963\n",
        "             (\"Abdul\", \"23\", \"5\", \"81\")]  # 1981\n",
        "\n",
        "raw_df = spark.createDataFrame(data_list).toDF(\"name\", \"day\", \"month\", \"year\")\n",
        "raw_df.printSchema()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- day: string (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- year: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0dOmyRMggFu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d726d462-6137-4ff5-d74b-3a5303b86c12"
      },
      "source": [
        "#Add a new column\n",
        "\n",
        "df1 = raw_df.withColumn(\"id\", monotonically_increasing_id())\n",
        "df1.show(10)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+-----+----+----------+\n",
            "| name|day|month|year|        id|\n",
            "+-----+---+-----+----+----------+\n",
            "| Ravi| 28|    1|2002|         0|\n",
            "|Abdul| 23|    5|  81|         1|\n",
            "| John| 12|   12|   6|8589934592|\n",
            "| Rosy|  7|    8|  63|8589934593|\n",
            "|Abdul| 23|    5|  81|8589934594|\n",
            "+-----+---+-----+----+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwuZUsnng87c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839f8801-ad1c-4d1d-a428-e814902a3269"
      },
      "source": [
        "# CASE WHEN and CAST\n",
        "df2=df1.withColumn(\"year\", expr(\"\"\"\n",
        "                                CASE WHEN year < 21  THEN cast(year as int) + 2000\n",
        "                                     WHEN year < 100 THEN cast(year as int) + 1900\n",
        "                                     ELSE cast(year as int)\n",
        "                                END\n",
        "                                \"\"\"))\n",
        "df2.show()\n",
        "\n",
        "df2=df1.withColumn(\"year\", expr(\"\"\"\n",
        "                                CASE WHEN year < 21  THEN year + 2000\n",
        "                                     WHEN year < 100 THEN year + 1900\n",
        "                                     ELSE year\n",
        "                                END\n",
        "                                \"\"\").cast(IntegerType()))\n",
        "df2.show()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+---+-----+----+----------+\n",
            "| name|day|month|year|        id|\n",
            "+-----+---+-----+----+----------+\n",
            "| Ravi| 28|    1|2002|         0|\n",
            "|Abdul| 23|    5|1981|         1|\n",
            "| John| 12|   12|2006|8589934592|\n",
            "| Rosy|  7|    8|1963|8589934593|\n",
            "|Abdul| 23|    5|1981|8589934594|\n",
            "+-----+---+-----+----+----------+\n",
            "\n",
            "+-----+---+-----+----+----------+\n",
            "| name|day|month|year|        id|\n",
            "+-----+---+-----+----+----------+\n",
            "| Ravi| 28|    1|2002|         0|\n",
            "|Abdul| 23|    5|1981|         1|\n",
            "| John| 12|   12|2006|8589934592|\n",
            "| Rosy|  7|    8|1963|8589934593|\n",
            "|Abdul| 23|    5|1981|8589934594|\n",
            "+-----+---+-----+----+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkcbiBpjOZL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c61a4a2d-1faf-4efc-9d4c-d33ffc1d7da5"
      },
      "source": [
        "# Cast the fields and alternative WHEN\n",
        "df3=df1.withColumn(\"day\", col(\"day\").cast(IntegerType())) \\\n",
        "       .withColumn(\"month\", col(\"month\").cast(IntegerType())) \\\n",
        "       .withColumn(\"year\", col(\"year\").cast(IntegerType())) \\\n",
        "\n",
        "df3.printSchema()\n",
        "\n",
        "df4=df3.withColumn(\"year\", \\\n",
        "                          when(col(\"year\") < 21, col(\"year\") + 2000) \\\n",
        "                        .when(col(\"year\") < 100, col(\"year\") + 1900) \\\n",
        "                        .otherwise(col(\"year\")))\n",
        "df4.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- month: integer (nullable = true)\n",
            " |-- year: integer (nullable = true)\n",
            " |-- id: long (nullable = false)\n",
            "\n",
            "+-----+---+-----+----+----------+\n",
            "| name|day|month|year|        id|\n",
            "+-----+---+-----+----+----------+\n",
            "| Ravi| 28|    1|2002|         0|\n",
            "|Abdul| 23|    5|1981|         1|\n",
            "| John| 12|   12|2006|8589934592|\n",
            "| Rosy|  7|    8|1963|8589934593|\n",
            "|Abdul| 23|    5|1981|8589934594|\n",
            "+-----+---+-----+----+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRGb1DXCS7Z9"
      },
      "source": [
        "**Add/Remove columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SSgyxRnS-gd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45b9ac6b-bd7d-400d-c051-ecc1ae174049"
      },
      "source": [
        "df5=df2.withColumn(\"dob\", expr(\"to_date(concat(day, '/', month, '/', year), 'd/M/y')\")) \\\n",
        "       .drop(\"day\", \"month\", \"year\") \\\n",
        "       .dropDuplicates([\"name\", \"dob\"])\n",
        "\n",
        "df5.sort(df5.dob.desc()).show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+----------+----------+\n",
            "| name|        id|       dob|\n",
            "+-----+----------+----------+\n",
            "| John|8589934592|2006-12-12|\n",
            "| Ravi|         0|2002-01-28|\n",
            "|Abdul|         1|1981-05-23|\n",
            "| Rosy|8589934593|1963-08-07|\n",
            "+-----+----------+----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlVmPJ3YXEBv"
      },
      "source": [
        "**Aggregation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3V5RWB5KXDZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e16e0217-68bc-4a94-ca89-dbd223aa46d8"
      },
      "source": [
        "from pyspark.sql import functions as f\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/invoices.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "invoice_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"invoices.csv\"))\n",
        "\n",
        "invoice_df.show(10)\n",
        "\n",
        "invoice_df.select(f.count(\"*\").alias(\"Count *\"),\n",
        "                  f.sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
        "                  f.avg(\"UnitPrice\").alias(\"AvgPrice\"),\n",
        "                  f.countDistinct(\"InvoiceNo\").alias(\"CountDistinct\")\n",
        "                  ).show()\n",
        "\n",
        "invoice_df.selectExpr(\n",
        "                  \"count(1) as `count 1`\",\n",
        "                  \"count(StockCode) as `count field`\",\n",
        "                  \"sum(Quantity) as TotalQuantity\",\n",
        "                  \"avg(UnitPrice) as AvgPrice\"\n",
        "                ).show()\n",
        "\n",
        "invoice_df.createOrReplaceTempView(\"sales\")\n",
        "summary_sql = spark.sql(\"\"\"\n",
        "      SELECT Country, InvoiceNo,\n",
        "            sum(Quantity) as TotalQuantity,\n",
        "            round(sum(Quantity*UnitPrice),2) as InvoiceValue\n",
        "      FROM sales\n",
        "      GROUP BY Country, InvoiceNo\"\"\")\n",
        "summary_sql.show()\n",
        "\n",
        "summary_df = invoice_df \\\n",
        "    .groupBy(\"Country\", \"InvoiceNo\") \\\n",
        "    .agg(f.sum(\"Quantity\").alias(\"TotalQuantity\"),\n",
        "         f.round(f.sum(f.expr(\"Quantity * UnitPrice\")), 2).alias(\"InvoiceValue\"),\n",
        "         f.expr(\"round(sum(Quantity * UnitPrice),2) as InvoiceValueExpr\")\n",
        "         )\n",
        "summary_df.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
            "|   536365|     null|WHITE HANGING HEA...|       6|01-12-2010 8.26|     2.55|     17850|United Kingdom|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
            "|   536365|   84406B|CREAM CUPID HEART...|       8|01-12-2010 8.26|     2.75|     17850|United Kingdom|\n",
            "|   536365|   84029G|KNITTED UNION FLA...|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
            "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
            "|   536365|    22752|SET 7 BABUSHKA NE...|       2|01-12-2010 8.26|     7.65|     17850|United Kingdom|\n",
            "|   536365|    21730|GLASS STAR FROSTE...|       6|01-12-2010 8.26|     4.25|     17850|United Kingdom|\n",
            "|   536366|    22633|HAND WARMER UNION...|       6|01-12-2010 8.28|     1.85|     17850|United Kingdom|\n",
            "|   536366|    22632|HAND WARMER RED P...|       6|01-12-2010 8.28|     1.85|     17850|United Kingdom|\n",
            "|   536367|    84879|ASSORTED COLOUR B...|      32|01-12-2010 8.34|     1.69|     13047|United Kingdom|\n",
            "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+-------+-------------+-----------------+-------------+\n",
            "|Count *|TotalQuantity|         AvgPrice|CountDistinct|\n",
            "+-------+-------------+-----------------+-------------+\n",
            "| 541909|      5176450|4.611113626088487|        25900|\n",
            "+-------+-------------+-----------------+-------------+\n",
            "\n",
            "+-------+-----------+-------------+----------------+\n",
            "|count 1|count field|TotalQuantity|        AvgPrice|\n",
            "+-------+-----------+-------------+----------------+\n",
            "| 541909|     541908|      5176450|4.61111362608295|\n",
            "+-------+-----------+-------------+----------------+\n",
            "\n",
            "+--------------+---------+-------------+------------+\n",
            "|       Country|InvoiceNo|TotalQuantity|InvoiceValue|\n",
            "+--------------+---------+-------------+------------+\n",
            "|United Kingdom|   536365|           40|      139.12|\n",
            "|United Kingdom|   536367|           83|      278.73|\n",
            "|United Kingdom|   536368|           15|       70.05|\n",
            "|United Kingdom|   536369|            3|       17.85|\n",
            "|        France|   536370|          449|      855.86|\n",
            "|United Kingdom|   536374|           32|       350.4|\n",
            "|United Kingdom|   536375|           88|      259.86|\n",
            "|United Kingdom|   536377|           12|        22.2|\n",
            "|United Kingdom|   536378|          454|      444.98|\n",
            "|United Kingdom|   536381|          198|      449.98|\n",
            "|United Kingdom|  C536379|           -1|       -27.5|\n",
            "|United Kingdom|   536382|          134|       430.6|\n",
            "|United Kingdom|  C536383|           -1|       -4.65|\n",
            "|United Kingdom|   536385|           53|      130.85|\n",
            "|United Kingdom|   536387|         1440|     3193.92|\n",
            "|United Kingdom|   536388|          108|      226.14|\n",
            "|     Australia|   536389|          107|      358.25|\n",
            "|United Kingdom|  C536391|         -132|     -141.48|\n",
            "|United Kingdom|   536397|           60|       279.0|\n",
            "|United Kingdom|   536400|           12|        17.4|\n",
            "+--------------+---------+-------------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+--------------+---------+-------------+------------+----------------+\n",
            "|       Country|InvoiceNo|TotalQuantity|InvoiceValue|InvoiceValueExpr|\n",
            "+--------------+---------+-------------+------------+----------------+\n",
            "|United Kingdom|   536365|           40|      139.12|          139.12|\n",
            "|United Kingdom|   536367|           83|      278.73|          278.73|\n",
            "|United Kingdom|   536368|           15|       70.05|           70.05|\n",
            "|United Kingdom|   536369|            3|       17.85|           17.85|\n",
            "|        France|   536370|          449|      855.86|          855.86|\n",
            "|United Kingdom|   536374|           32|       350.4|           350.4|\n",
            "|United Kingdom|   536375|           88|      259.86|          259.86|\n",
            "|United Kingdom|   536377|           12|        22.2|            22.2|\n",
            "|United Kingdom|   536378|          454|      444.98|          444.98|\n",
            "|United Kingdom|   536381|          198|      449.98|          449.98|\n",
            "|United Kingdom|  C536379|           -1|       -27.5|           -27.5|\n",
            "|United Kingdom|   536382|          134|       430.6|           430.6|\n",
            "|United Kingdom|  C536383|           -1|       -4.65|           -4.65|\n",
            "|United Kingdom|   536385|           53|      130.85|          130.85|\n",
            "|United Kingdom|   536387|         1440|     3193.92|         3193.92|\n",
            "|United Kingdom|   536388|          108|      226.14|          226.14|\n",
            "|     Australia|   536389|          107|      358.25|          358.25|\n",
            "|United Kingdom|  C536391|         -132|     -141.48|         -141.48|\n",
            "|United Kingdom|   536397|           60|       279.0|           279.0|\n",
            "|United Kingdom|   536400|           12|        17.4|            17.4|\n",
            "+--------------+---------+-------------+------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w5GLlrhag5r"
      },
      "source": [
        "**Group By**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDA-d543aNUz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adb6d1ff-5f4b-4b03-f9a8-9f7c1607f147"
      },
      "source": [
        "from pyspark.sql import functions as f\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/invoices.csv'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "invoice_df=spark.read \\\n",
        "                    .format(\"csv\") \\\n",
        "                    .option(\"header\", \"true\") \\\n",
        "                    .option(\"mode\", \"FAILFAST\") \\\n",
        "                    .option(\"inferSchema\", \"true\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"invoices.csv\"))\n",
        "\n",
        "invoice_df.show(10)\n",
        "\n",
        "NumInvoices = f.countDistinct(\"InvoiceNo\").alias(\"NumInvoices\")\n",
        "TotalQuantity = f.sum(\"Quantity\").alias(\"TotalQuantity\")\n",
        "InvoiceValue = f.expr(\"round(sum(Quantity * UnitPrice),2) as InvoiceValue\")\n",
        "\n",
        "exSummary_df = invoice_df \\\n",
        "    .withColumn(\"InvoiceDate\", f.to_date(f.col(\"InvoiceDate\"), \"dd-MM-yyyy H.mm\")) \\\n",
        "    .where(\"year(InvoiceDate) == 2010\") \\\n",
        "    .withColumn(\"WeekNumber\", f.weekofyear(f.col(\"InvoiceDate\"))) \\\n",
        "    .groupBy(\"Country\", \"WeekNumber\") \\\n",
        "    .agg(NumInvoices, TotalQuantity, InvoiceValue)\n",
        "\n",
        "exSummary_df.sort(\"Country\", \"WeekNumber\").show()\n",
        "\n",
        "exSummary_df.coalesce(1) \\\n",
        "    .write \\\n",
        "    .format(\"parquet\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .save(\"output\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
            "|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n",
            "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
            "|   536365|     null|WHITE HANGING HEA...|       6|01-12-2010 8.26|     2.55|     17850|United Kingdom|\n",
            "|   536365|    71053| WHITE METAL LANTERN|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
            "|   536365|   84406B|CREAM CUPID HEART...|       8|01-12-2010 8.26|     2.75|     17850|United Kingdom|\n",
            "|   536365|   84029G|KNITTED UNION FLA...|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
            "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n",
            "|   536365|    22752|SET 7 BABUSHKA NE...|       2|01-12-2010 8.26|     7.65|     17850|United Kingdom|\n",
            "|   536365|    21730|GLASS STAR FROSTE...|       6|01-12-2010 8.26|     4.25|     17850|United Kingdom|\n",
            "|   536366|    22633|HAND WARMER UNION...|       6|01-12-2010 8.28|     1.85|     17850|United Kingdom|\n",
            "|   536366|    22632|HAND WARMER RED P...|       6|01-12-2010 8.28|     1.85|     17850|United Kingdom|\n",
            "|   536367|    84879|ASSORTED COLOUR B...|      32|01-12-2010 8.34|     1.69|     13047|United Kingdom|\n",
            "+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "+---------------+----------+-----------+-------------+------------+\n",
            "|        Country|WeekNumber|NumInvoices|TotalQuantity|InvoiceValue|\n",
            "+---------------+----------+-----------+-------------+------------+\n",
            "|      Australia|        48|          1|          107|      358.25|\n",
            "|      Australia|        49|          1|          214|       258.9|\n",
            "|      Australia|        50|          2|          133|      387.95|\n",
            "|        Austria|        50|          2|            3|      257.04|\n",
            "|        Bahrain|        51|          1|           54|      205.74|\n",
            "|        Belgium|        48|          1|          528|       346.1|\n",
            "|        Belgium|        50|          2|          285|      625.16|\n",
            "|        Belgium|        51|          2|          942|      838.65|\n",
            "|Channel Islands|        49|          1|           80|      363.53|\n",
            "|         Cyprus|        50|          1|          917|     1590.82|\n",
            "|        Denmark|        49|          1|          454|      1281.5|\n",
            "|           EIRE|        48|          7|         2822|     3147.23|\n",
            "|           EIRE|        49|          5|         1280|      3284.1|\n",
            "|           EIRE|        50|          5|         1184|     2321.78|\n",
            "|           EIRE|        51|          5|           95|      276.84|\n",
            "|        Finland|        50|          1|         1254|       892.8|\n",
            "|         France|        48|          4|         1299|     2808.16|\n",
            "|         France|        49|          9|         2303|     4527.01|\n",
            "|         France|        50|          6|          529|      537.32|\n",
            "|         France|        51|          5|          847|     1702.87|\n",
            "+---------------+----------+-----------+-------------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPHwGNntbjl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1503e30c-ae43-4ffe-d811-79ea32db65e8"
      },
      "source": [
        "!ls -l output/"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 2499 Apr 13 06:03 part-00000-371fba1a-007f-4447-bbfe-44310fbac52c-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root    0 Apr 13 06:03 _SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBeNrPR0ccQc"
      },
      "source": [
        "**Windowing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odcF10DwcgIY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a3db61-e456-4207-f511-7660e5a7ed63"
      },
      "source": [
        "from pyspark.sql import functions as f\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/summary.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "summary_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"summary.parquet\"))\n",
        "\n",
        "summary_df.sort(\"Country\", \"WeekNumber\").show()\n",
        "\n",
        "running_total_window = Window.partitionBy(\"Country\") \\\n",
        "    .orderBy(\"WeekNumber\") \\\n",
        "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "summary_df.withColumn(\"RunningTotal\",\n",
        "                      f.sum(\"InvoiceValue\").over(running_total_window)) \\\n",
        "    .sort(\"Country\", \"WeekNumber\").show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+----------+-----------+-------------+------------+\n",
            "|        Country|WeekNumber|NumInvoices|TotalQuantity|InvoiceValue|\n",
            "+---------------+----------+-----------+-------------+------------+\n",
            "|      Australia|        48|          1|          107|      358.25|\n",
            "|      Australia|        49|          1|          214|       258.9|\n",
            "|      Australia|        50|          2|          133|      387.95|\n",
            "|        Austria|        50|          2|            3|      257.04|\n",
            "|        Bahrain|        51|          1|           54|      205.74|\n",
            "|        Belgium|        48|          1|          528|       346.1|\n",
            "|        Belgium|        50|          2|          285|      625.16|\n",
            "|        Belgium|        51|          2|          942|      838.65|\n",
            "|Channel Islands|        49|          1|           80|      363.53|\n",
            "|         Cyprus|        50|          1|          917|     1590.82|\n",
            "|        Denmark|        49|          1|          454|      1281.5|\n",
            "|           EIRE|        48|          7|         2822|     3147.23|\n",
            "|           EIRE|        49|          5|         1280|      3284.1|\n",
            "|           EIRE|        50|          5|         1184|     2321.78|\n",
            "|           EIRE|        51|          5|           95|      276.84|\n",
            "|        Finland|        50|          1|         1254|       892.8|\n",
            "|         France|        48|          4|         1299|     2808.16|\n",
            "|         France|        49|          9|         2303|     4527.01|\n",
            "|         France|        50|          6|          529|      537.32|\n",
            "|         France|        51|          5|          847|     1702.87|\n",
            "+---------------+----------+-----------+-------------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---------------+----------+-----------+-------------+------------+------------------+\n",
            "|        Country|WeekNumber|NumInvoices|TotalQuantity|InvoiceValue|      RunningTotal|\n",
            "+---------------+----------+-----------+-------------+------------+------------------+\n",
            "|      Australia|        48|          1|          107|      358.25|            358.25|\n",
            "|      Australia|        49|          1|          214|       258.9|            617.15|\n",
            "|      Australia|        50|          2|          133|      387.95|1005.0999999999999|\n",
            "|        Austria|        50|          2|            3|      257.04|            257.04|\n",
            "|        Bahrain|        51|          1|           54|      205.74|            205.74|\n",
            "|        Belgium|        48|          1|          528|       346.1|             346.1|\n",
            "|        Belgium|        50|          2|          285|      625.16|            971.26|\n",
            "|        Belgium|        51|          2|          942|      838.65|1809.9099999999999|\n",
            "|Channel Islands|        49|          1|           80|      363.53|            363.53|\n",
            "|         Cyprus|        50|          1|          917|     1590.82|           1590.82|\n",
            "|        Denmark|        49|          1|          454|      1281.5|            1281.5|\n",
            "|           EIRE|        48|          7|         2822|     3147.23|           3147.23|\n",
            "|           EIRE|        49|          5|         1280|      3284.1|           6431.33|\n",
            "|           EIRE|        50|          5|         1184|     2321.78|           8753.11|\n",
            "|           EIRE|        51|          5|           95|      276.84|           9029.95|\n",
            "|        Finland|        50|          1|         1254|       892.8|             892.8|\n",
            "|         France|        48|          4|         1299|     2808.16|           2808.16|\n",
            "|         France|        49|          9|         2303|     4527.01|           7335.17|\n",
            "|         France|        50|          6|          529|      537.32|           7872.49|\n",
            "|         France|        51|          5|          847|     1702.87|           9575.36|\n",
            "+---------------+----------+-----------+-------------+------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSB3kYwMge77"
      },
      "source": [
        "**Ranking**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0GLi9jMgguQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6d210d-5c62-4e15-831c-be38e900b961"
      },
      "source": [
        "from pyspark.sql import functions as f\n",
        "url='https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/summary.parquet'\n",
        "\n",
        "spark.sparkContext.addFile(url)\n",
        "\n",
        "summary_df=spark.read \\\n",
        "                    .format(\"parquet\") \\\n",
        "                    .load('file://'+SparkFiles.get(\"summary.parquet\"))\n",
        "\n",
        "summary_df.sort(\"Country\", \"WeekNumber\").show()\n",
        "\n",
        "rank_window = Window.partitionBy(\"Country\") \\\n",
        "        .orderBy(f.col(\"InvoiceValue\").desc()) \\\n",
        "        .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "df = summary_df.withColumn(\"Rank\", f.dense_rank().over(rank_window)) \\\n",
        "    .where(f.col(\"Rank\") <= 2) \\\n",
        "    .sort(\"Country\", \"Rank\") \\\n",
        "    .show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------+----------+-----------+-------------+------------+\n",
            "|        Country|WeekNumber|NumInvoices|TotalQuantity|InvoiceValue|\n",
            "+---------------+----------+-----------+-------------+------------+\n",
            "|      Australia|        48|          1|          107|      358.25|\n",
            "|      Australia|        49|          1|          214|       258.9|\n",
            "|      Australia|        50|          2|          133|      387.95|\n",
            "|        Austria|        50|          2|            3|      257.04|\n",
            "|        Bahrain|        51|          1|           54|      205.74|\n",
            "|        Belgium|        48|          1|          528|       346.1|\n",
            "|        Belgium|        50|          2|          285|      625.16|\n",
            "|        Belgium|        51|          2|          942|      838.65|\n",
            "|Channel Islands|        49|          1|           80|      363.53|\n",
            "|         Cyprus|        50|          1|          917|     1590.82|\n",
            "|        Denmark|        49|          1|          454|      1281.5|\n",
            "|           EIRE|        48|          7|         2822|     3147.23|\n",
            "|           EIRE|        49|          5|         1280|      3284.1|\n",
            "|           EIRE|        50|          5|         1184|     2321.78|\n",
            "|           EIRE|        51|          5|           95|      276.84|\n",
            "|        Finland|        50|          1|         1254|       892.8|\n",
            "|         France|        48|          4|         1299|     2808.16|\n",
            "|         France|        49|          9|         2303|     4527.01|\n",
            "|         France|        50|          6|          529|      537.32|\n",
            "|         France|        51|          5|          847|     1702.87|\n",
            "+---------------+----------+-----------+-------------+------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+---------------+----------+-----------+-------------+------------+----+\n",
            "|        Country|WeekNumber|NumInvoices|TotalQuantity|InvoiceValue|Rank|\n",
            "+---------------+----------+-----------+-------------+------------+----+\n",
            "|      Australia|        50|          2|          133|      387.95|   1|\n",
            "|      Australia|        48|          1|          107|      358.25|   2|\n",
            "|        Austria|        50|          2|            3|      257.04|   1|\n",
            "|        Bahrain|        51|          1|           54|      205.74|   1|\n",
            "|        Belgium|        51|          2|          942|      838.65|   1|\n",
            "|        Belgium|        50|          2|          285|      625.16|   2|\n",
            "|Channel Islands|        49|          1|           80|      363.53|   1|\n",
            "|         Cyprus|        50|          1|          917|     1590.82|   1|\n",
            "|        Denmark|        49|          1|          454|      1281.5|   1|\n",
            "|           EIRE|        49|          5|         1280|      3284.1|   1|\n",
            "|           EIRE|        48|          7|         2822|     3147.23|   2|\n",
            "|        Finland|        50|          1|         1254|       892.8|   1|\n",
            "|         France|        49|          9|         2303|     4527.01|   1|\n",
            "|         France|        48|          4|         1299|     2808.16|   2|\n",
            "|        Germany|        50|         15|         1973|     5065.79|   1|\n",
            "|        Germany|        49|         12|         1852|     4521.39|   2|\n",
            "|        Iceland|        49|          1|          319|      711.79|   1|\n",
            "|         Israel|        50|          1|          -56|     -227.44|   1|\n",
            "|          Italy|        48|          1|          164|       427.8|   1|\n",
            "|          Italy|        51|          1|          131|       383.7|   2|\n",
            "+---------------+----------+-----------+-------------+------------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPM5t3mHjEtI"
      },
      "source": [
        "**Joins**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_BFVvHBjGwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a84901-904b-48bc-8511-f0fface31d64"
      },
      "source": [
        "    orders_list = [(\"01\", \"02\", 350, 1),\n",
        "                   (\"01\", \"04\", 580, 1),\n",
        "                   (\"01\", \"07\", 320, 2),\n",
        "                   (\"02\", \"03\", 450, 1),\n",
        "                   (\"02\", \"06\", 220, 1),\n",
        "                   (\"03\", \"01\", 195, 1),\n",
        "                   (\"04\", \"09\", 270, 3),\n",
        "                   (\"04\", \"08\", 410, 2),\n",
        "                   (\"05\", \"02\", 350, 1)]\n",
        "\n",
        "    order_df = spark.createDataFrame(orders_list).toDF(\"order_id\", \"prod_id\", \"unit_price\", \"qty\")\n",
        "\n",
        "    product_list = [(\"01\", \"Scroll Mouse\", 250, 20),\n",
        "                    (\"02\", \"Optical Mouse\", 350, 20),\n",
        "                    (\"03\", \"Wireless Mouse\", 450, 50),\n",
        "                    (\"04\", \"Wireless Keyboard\", 580, 50),\n",
        "                    (\"05\", \"Standard Keyboard\", 360, 10),\n",
        "                    (\"06\", \"16 GB Flash Storage\", 240, 100),\n",
        "                    (\"07\", \"32 GB Flash Storage\", 320, 50),\n",
        "                    (\"08\", \"64 GB Flash Storage\", 430, 25)]\n",
        "\n",
        "    product_df = spark.createDataFrame(product_list).toDF(\"prod_id\", \"prod_name\", \"list_price\", \"qty\")\n",
        "\n",
        "    product_df.show()\n",
        "    order_df.show()\n",
        "\n",
        "    join_expr = order_df.prod_id == product_df.prod_id\n",
        "\n",
        "    product_renamed_df = product_df.withColumnRenamed(\"qty\", \"reorder_qty\")\n",
        "\n",
        "    order_df.join(product_renamed_df, join_expr, \"inner\") \\\n",
        "        .drop(product_renamed_df.prod_id) \\\n",
        "        .select(\"order_id\", \"prod_id\", \"prod_name\", \"unit_price\", \"list_price\", \"qty\") \\\n",
        "        .show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------------------+----------+---+\n",
            "|prod_id|          prod_name|list_price|qty|\n",
            "+-------+-------------------+----------+---+\n",
            "|     01|       Scroll Mouse|       250| 20|\n",
            "|     02|      Optical Mouse|       350| 20|\n",
            "|     03|     Wireless Mouse|       450| 50|\n",
            "|     04|  Wireless Keyboard|       580| 50|\n",
            "|     05|  Standard Keyboard|       360| 10|\n",
            "|     06|16 GB Flash Storage|       240|100|\n",
            "|     07|32 GB Flash Storage|       320| 50|\n",
            "|     08|64 GB Flash Storage|       430| 25|\n",
            "+-------+-------------------+----------+---+\n",
            "\n",
            "+--------+-------+----------+---+\n",
            "|order_id|prod_id|unit_price|qty|\n",
            "+--------+-------+----------+---+\n",
            "|      01|     02|       350|  1|\n",
            "|      01|     04|       580|  1|\n",
            "|      01|     07|       320|  2|\n",
            "|      02|     03|       450|  1|\n",
            "|      02|     06|       220|  1|\n",
            "|      03|     01|       195|  1|\n",
            "|      04|     09|       270|  3|\n",
            "|      04|     08|       410|  2|\n",
            "|      05|     02|       350|  1|\n",
            "+--------+-------+----------+---+\n",
            "\n",
            "+--------+-------+-------------------+----------+----------+---+\n",
            "|order_id|prod_id|          prod_name|unit_price|list_price|qty|\n",
            "+--------+-------+-------------------+----------+----------+---+\n",
            "|      03|     01|       Scroll Mouse|       195|       250|  1|\n",
            "|      01|     04|  Wireless Keyboard|       580|       580|  1|\n",
            "|      02|     06|16 GB Flash Storage|       220|       240|  1|\n",
            "|      01|     07|32 GB Flash Storage|       320|       320|  2|\n",
            "|      01|     02|      Optical Mouse|       350|       350|  1|\n",
            "|      05|     02|      Optical Mouse|       350|       350|  1|\n",
            "|      02|     03|     Wireless Mouse|       450|       450|  1|\n",
            "|      04|     08|64 GB Flash Storage|       410|       430|  2|\n",
            "+--------+-------+-------------------+----------+----------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GojGKmAjUWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4da0ca2-7e63-4e76-aa89-85cc210b77f1"
      },
      "source": [
        "    orders_list = [(\"01\", \"02\", 350, 1),\n",
        "                   (\"01\", \"04\", 580, 1),\n",
        "                   (\"01\", \"07\", 320, 2),\n",
        "                   (\"02\", \"03\", 450, 1),\n",
        "                   (\"02\", \"06\", 220, 1),\n",
        "                   (\"03\", \"01\", 195, 1),\n",
        "                   (\"04\", \"09\", 270, 3),\n",
        "                   (\"04\", \"08\", 410, 2),\n",
        "                   (\"05\", \"02\", 350, 1)]\n",
        "\n",
        "    order_df = spark.createDataFrame(orders_list).toDF(\"order_id\", \"prod_id\", \"unit_price\", \"qty\")\n",
        "\n",
        "    product_list = [(\"01\", \"Scroll Mouse\", 250, 20),\n",
        "                    (\"02\", \"Optical Mouse\", 350, 20),\n",
        "                    (\"03\", \"Wireless Mouse\", 450, 50),\n",
        "                    (\"04\", \"Wireless Keyboard\", 580, 50),\n",
        "                    (\"05\", \"Standard Keyboard\", 360, 10),\n",
        "                    (\"06\", \"16 GB Flash Storage\", 240, 100),\n",
        "                    (\"07\", \"32 GB Flash Storage\", 320, 50),\n",
        "                    (\"08\", \"64 GB Flash Storage\", 430, 25)]\n",
        "\n",
        "    product_df = spark.createDataFrame(product_list).toDF(\"prod_id\", \"prod_name\", \"list_price\", \"qty\")\n",
        "\n",
        "    product_df.show()\n",
        "    order_df.show()\n",
        "\n",
        "    join_expr = order_df.prod_id == product_df.prod_id\n",
        "\n",
        "    product_renamed_df = product_df.withColumnRenamed(\"qty\", \"reorder_qty\")\n",
        "\n",
        "    order_df.join(product_renamed_df, join_expr, \"left\") \\\n",
        "        .drop(product_renamed_df.prod_id) \\\n",
        "        .select(\"order_id\", \"prod_id\", \"prod_name\", \"unit_price\", \"list_price\", \"qty\") \\\n",
        "        .withColumn(\"prod_name\", expr(\"coalesce(prod_name, prod_id)\")) \\\n",
        "        .withColumn(\"list_price\", expr(\"coalesce(list_price, unit_price)\")) \\\n",
        "        .sort(\"order_id\") \\\n",
        "        .show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-------------------+----------+---+\n",
            "|prod_id|          prod_name|list_price|qty|\n",
            "+-------+-------------------+----------+---+\n",
            "|     01|       Scroll Mouse|       250| 20|\n",
            "|     02|      Optical Mouse|       350| 20|\n",
            "|     03|     Wireless Mouse|       450| 50|\n",
            "|     04|  Wireless Keyboard|       580| 50|\n",
            "|     05|  Standard Keyboard|       360| 10|\n",
            "|     06|16 GB Flash Storage|       240|100|\n",
            "|     07|32 GB Flash Storage|       320| 50|\n",
            "|     08|64 GB Flash Storage|       430| 25|\n",
            "+-------+-------------------+----------+---+\n",
            "\n",
            "+--------+-------+----------+---+\n",
            "|order_id|prod_id|unit_price|qty|\n",
            "+--------+-------+----------+---+\n",
            "|      01|     02|       350|  1|\n",
            "|      01|     04|       580|  1|\n",
            "|      01|     07|       320|  2|\n",
            "|      02|     03|       450|  1|\n",
            "|      02|     06|       220|  1|\n",
            "|      03|     01|       195|  1|\n",
            "|      04|     09|       270|  3|\n",
            "|      04|     08|       410|  2|\n",
            "|      05|     02|       350|  1|\n",
            "+--------+-------+----------+---+\n",
            "\n",
            "+--------+-------+-------------------+----------+----------+---+\n",
            "|order_id|prod_id|          prod_name|unit_price|list_price|qty|\n",
            "+--------+-------+-------------------+----------+----------+---+\n",
            "|      01|     04|  Wireless Keyboard|       580|       580|  1|\n",
            "|      01|     07|32 GB Flash Storage|       320|       320|  2|\n",
            "|      01|     02|      Optical Mouse|       350|       350|  1|\n",
            "|      02|     03|     Wireless Mouse|       450|       450|  1|\n",
            "|      02|     06|16 GB Flash Storage|       220|       240|  1|\n",
            "|      03|     01|       Scroll Mouse|       195|       250|  1|\n",
            "|      04|     09|                 09|       270|       270|  3|\n",
            "|      04|     08|64 GB Flash Storage|       410|       430|  2|\n",
            "|      05|     02|      Optical Mouse|       350|       350|  1|\n",
            "+--------+-------+-------------------+----------+----------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8i_h5wABJ6-"
      },
      "source": [
        "**Shuffle Join**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jqg_06_GBMrb"
      },
      "source": [
        "!mkdir data data/d1 data/d2\n",
        "!wget -O data/d1/part-00000-00af64b6-7ef5-4909-8f82-b8897114efaf-c000.json https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/d1/part-00000-00af64b6-7ef5-4909-8f82-b8897114efaf-c000.json > /dev/null 2>&1\n",
        "!wget -O data/d1/part-00001-00af64b6-7ef5-4909-8f82-b8897114efaf-c000.json https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/d1/part-00001-00af64b6-7ef5-4909-8f82-b8897114efaf-c000.json > /dev/null 2>&1\n",
        "!wget -O data/d1/part-00002-00af64b6-7ef5-4909-8f82-b8897114efaf-c000.json https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/d1/part-00002-00af64b6-7ef5-4909-8f82-b8897114efaf-c000.json > /dev/null 2>&1 \n",
        "\n",
        "!wget -O data/d2/part-00000-ee8814fd-bbb7-4a16-912b-f74b04a6d4fa-c000.json https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/Spark-Programming-In-Python-master/19-ShuffleJoinDemo/data/d2/part-00000-ee8814fd-bbb7-4a16-912b-f74b04a6d4fa-c000.json > /dev/null 2>&1\n",
        "!wget -O data/d2/part-00001-ee8814fd-bbb7-4a16-912b-f74b04a6d4fa-c000.json https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/d1/part-00001-ee8814fd-bbb7-4a16-912b-f74b04a6d4fa-c000.json > /dev/null 2>&1\n",
        "!wget -O data/d2/part-00002-ee8814fd-bbb7-4a16-912b-f74b04a6d4fa-c000.json https://raw.githubusercontent.com/sku1978/sk-share-repo/main/Spark/SparkDataFrame/data/d1/part-00002-ee8814fd-bbb7-4a16-912b-f74b04a6d4fa-c000.json > /dev/null 2>&1"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ixdg6tPyBmVb",
        "outputId": "be0698cd-e7a0-460d-c620-bec5cc571966"
      },
      "source": [
        "flight_time_df1 = spark.read.json(\"data/d1/\")\n",
        "flight_time_df2 = spark.read.json(\"data/d2/\")\n",
        "\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 3)\n",
        "\n",
        "join_expr = flight_time_df1.id == flight_time_df2.id\n",
        "join_df = flight_time_df1.join(flight_time_df2, join_expr, \"inner\")\n",
        "join_df.explain()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "== Physical Plan ==\n",
            "*(5) SortMergeJoin [id#2812L], [id#2836L], Inner\n",
            ":- *(2) Sort [id#2812L ASC NULLS FIRST], false, 0\n",
            ":  +- Exchange hashpartitioning(id#2812L, 3), ENSURE_REQUIREMENTS, [id=#1185]\n",
            ":     +- *(1) Filter isnotnull(id#2812L)\n",
            ":        +- FileScan json [DEST#2805,DEST_CITY_NAME#2806,FL_DATE#2807,OP_CARRIER#2808,OP_CARRIER_FL_NUM#2809L,ORIGIN#2810,ORIGIN_CITY_NAME#2811,id#2812L] Batched: false, DataFilters: [isnotnull(id#2812L)], Format: JSON, Location: InMemoryFileIndex[file:/content/data/d1], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<DEST:string,DEST_CITY_NAME:string,FL_DATE:string,OP_CARRIER:string,OP_CARRIER_FL_NUM:bigin...\n",
            "+- *(4) Sort [id#2836L ASC NULLS FIRST], false, 0\n",
            "   +- Exchange hashpartitioning(id#2836L, 3), ENSURE_REQUIREMENTS, [id=#1193]\n",
            "      +- *(3) Filter isnotnull(id#2836L)\n",
            "         +- FileScan json [ARR_TIME#2828L,CANCELLED#2829L,CRS_ARR_TIME#2830L,CRS_DEP_TIME#2831L,DEP_TIME#2832L,DISTANCE#2833L,TAXI_IN#2834L,WHEELS_ON#2835L,id#2836L] Batched: false, DataFilters: [isnotnull(id#2836L)], Format: JSON, Location: InMemoryFileIndex[file:/content/data/d2], PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<ARR_TIME:bigint,CANCELLED:bigint,CRS_ARR_TIME:bigint,CRS_DEP_TIME:bigint,DEP_TIME:bigint,D...\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piuP7dIZXBM5"
      },
      "source": [
        "**View Log**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRtr-fRE5_3w"
      },
      "source": [
        "!cat app-logs/sparklog.log"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}